{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# for colab\n! pip install kekas albumentations"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# This method pre-populates the outputframe with the configuration that Plotly \n# expects and must be executed for every cell which is displaying a Plotly graph.\ndef enable_plotly_in_cell():\n    import IPython\n    from plotly.offline import init_notebook_mode\n    display(IPython.core.display.HTML('''\n          <script src=\"/static/components/requirejs/require.js\"></script>\n    '''))\n    init_notebook_mode(connected=False)"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"import os\nfrom pathlib import Path\n\nimport cv2\n\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nimport torchvision\nfrom torchvision import transforms\n\nfrom albumentations import Compose, JpegCompression, CLAHE, RandomRotate90, Transpose, ShiftScaleRotate, \\\n        Blur, OpticalDistortion, GridDistortion, HueSaturationValue, Flip, VerticalFlip\n\nfrom kekas import Keker, DataOwner, DataKek\nfrom kekas.transformations import Transformer, to_torch, normalize\nfrom kekas.metrics import accuracy\nfrom kekas.modules import Flatten, AdaptiveConcatPool2d\nfrom kekas.callbacks import Callback, Callbacks, DebuggerCallback"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# OPTIONAL\n# Specify ids of GPU's\n# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""},{"cell_type":"markdown","metadata":{},"source":["# Dataset creation"]},{"cell_type":"markdown","metadata":{},"source":["## Downloading"]},{"cell_type":"markdown","metadata":{},"source":["Let's see how to build a classification pipeline with Kekas.\n","We will finetune a convolutional neural network on Stanford Dogs dataset (https://www.kaggle.com/jessicali9530/stanford-dogs-dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# Download and extract images\n! wget http://vision.stanford.edu/aditya86/ImageNetDogs/images.tar\n! tar -xf ./images.tar &>/dev/null"},{"cell_type":"markdown","metadata":{},"source":["## Dataframe creation and train/val split"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# Let's create a pandas DataFrame to help us with data handling\nroot_dir = Path(\"./Images\")  # path to dataset root directory\n\n# create a mapping from directory name to label index\nlabel2index = {n.name: i for i, n in enumerate(sorted(root_dir.iterdir()))}\nlabel2index\nfpaths = []\nlabels = []\nfor d in root_dir.iterdir():\n    for f in d.iterdir():\n        labels.append(label2index[d.name])\n        fpaths.append(str(f))\n\ndf = pd.DataFrame(data={\"fpath\": fpaths, \"label\": labels})\ndf.head()"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# split dataset to train and val parts\ntrain_df, val_df = train_test_split(df, test_size=2000)\ntrain_df.shape, val_df.shape"},{"cell_type":"markdown","metadata":{},"source":["## Augmentations"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# create train and val datasets using DataKek class - a pytorch Dataset that uses pandas DataFrame as data source\n\n# at first we need to create a reader function that will define how image will be opened\ndef reader_fn(i, row):\n    # it always gets i and row as parameters\n    # where i is an index of dataframe and row is a dataframes row\n    image = cv2.imread(row[\"fpath\"])[:,:,::-1]  # BGR -> RGB\n    label = row[\"label\"]\n    return {\"image\": image, \"label\": label}\n\n\n# Then we should create transformations/augmentations\n# We will use awesome https://github.com/albu/albumentations library\ndef augs(p=0.5):\n    return Compose([\n        CLAHE(),\n        RandomRotate90(),\n        Transpose(),\n        ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.50, rotate_limit=45, p=.75),\n        Blur(blur_limit=3),\n        OpticalDistortion(),\n        GridDistortion(),\n        HueSaturationValue()\n    ], p=p)\n\ndef get_transforms(dataset_key, size, p):\n    # we need to use a Transformer class to apply transformations to DataKeks elements\n    # dataset_key is an image key in dict returned by reader_fn\n    \n    PRE_TFMS = Transformer(dataset_key, lambda x: cv2.resize(x, (size, size)))\n\n    AUGS = Transformer(dataset_key, lambda x: augs()(image=x)[\"image\"])\n\n    NRM_TFMS = transforms.Compose([\n        Transformer(dataset_key, to_torch()),\n        Transformer(dataset_key, normalize())\n    ])\n    \n    train_tfms = transforms.Compose([PRE_TFMS, AUGS, NRM_TFMS])\n    val_tfms = transforms.Compose([PRE_TFMS, NRM_TFMS])  # because we don't want to augment val set yet\n    \n    return train_tfms, val_tfms"},{"cell_type":"markdown","metadata":{},"source":["## DataKeks creation"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# now let's create DataKeks\ntrain_tfms, val_tfms = get_transforms(\"image\", 224, 0.5)\n\ntrain_dk = DataKek(df=train_df, reader_fn=reader_fn, transforms=train_tfms)\nval_dk = DataKek(df=val_df, reader_fn=reader_fn, transforms=val_tfms)"},{"cell_type":"markdown","metadata":{},"source":["## DataLoaders"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# and DataLoaders\nbatch_size = 16\nworkers = 4\n\ntrain_dl = DataLoader(train_dk, batch_size=batch_size, num_workers=workers, shuffle=True, drop_last=True)\nval_dl = DataLoader(val_dk, batch_size=batch_size, num_workers=workers, shuffle=False)"},{"cell_type":"markdown","metadata":{},"source":["# Model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# create a simple neural network using pretrainedmodels library\n# https://github.com/Cadene/pretrained-models.pytorch\n\nclass Net(nn.Module):\n    def __init__(self,\n                num_classes: int,\n                p: float = 0.5,\n                pooling_size: int = 2,\n                last_conv_size: int = 512,\n                arch: str = \"resnet18\",\n                pretrained: bool = True) -> None:\n        \"\"\"A simple model to finetune.\n        \n        Args:\n            num_classes: the number of target classes, the size of the last layer's output\n            p: dropout probability\n            pooling_size: the size of the result feature map after adaptive pooling layer\n            last_conv_size: size of the flatten last backbone conv layer\n            arch: the name of the architecture form pretrainedmodels\n            pretrained: the mode for pretrained model from pretrainedmodels\n        \"\"\"\n        super().__init__()\n        net = torchvision.models.__dict__[arch](pretrained=pretrained)\n        modules = list(net.children())[:-2]  # delete last layers: pooling and linear\n        \n        # add custom head\n        modules += [nn.Sequential(\n            # AdaptiveConcatPool2d is a concat of AdaptiveMaxPooling and AdaptiveAveragePooling \n            AdaptiveConcatPool2d(size=pooling_size),\n            Flatten(),\n            nn.BatchNorm1d(2 * pooling_size * pooling_size * last_conv_size),\n            nn.Dropout(p),\n            nn.Linear(2 * pooling_size * pooling_size * last_conv_size, num_classes)\n        )]\n        self.net = nn.Sequential(*modules)\n\n    def forward(self, x):\n        logits = self.net(x)\n        return logits"},{"cell_type":"markdown","metadata":{},"source":["# Keker"]},{"cell_type":"markdown","metadata":{},"source":["## Initialization"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# the three whales of your pipelane are: the data, the model and the loss (hi, Jeremy)\n\n# the data is represented in Kekas by DataOwner. It is a namedtuple with three fields:\n# 'train_dl', 'val_dl', 'test_dl'\n# For training process we will need at least two of them, and we can skip 'test_dl' for now\n# so we will initialize it with `None` value.\ndataowner = DataOwner(train_dl, val_dl, None)\n\n# model is just a pytorch nn.Module, that we created vefore\nmodel = Net(num_classes=len(label2index))  # 120 classes\n\n# loss or criterion is also a pytorch nn.Module. For multiloss scenarios it can be a list of nn.Modules\n# for our simple example let's use the standart cross entopy criterion\ncriterion = nn.CrossEntropyLoss()"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# Also we need to specify, what model will do with each batch of data on each iteration\n# We should define a `step_fn` function\n# The code below repeats a `keker.default_step_fn` code to provide you with a concept of step function\n\ndef step_fn(model: torch.nn.Module,\n            batch: torch.Tensor) -> torch.Tensor:\n    \"\"\"Determine what your model will do with your data.\n\n    Args:\n        model: the pytorch module to pass input in\n        batch: the batch of data from the DataLoader\n\n    Returns:\n        The models forward pass results\n    \"\"\"\n    \n    # you could define here whatever logic you want\n    inp = batch[\"image\"]  # here we get an \"image\" from our dataset\n    return model(inp)"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# previous preparations was mostly out of scope of Kekas library (except DataKeks creation)\n# Now let's dive into kekas a little bit\n\n# firstly, we create a Keker - the core Kekas class, that provides all the keks for your pipeline\nkeker = Keker(model=model,\n              dataowner=dataowner,\n              criterion=criterion,\n              step_fn=step_fn,                    # previosly defined step function\n              target_key=\"label\",                 # remember, we defined it in the reader_fn for DataKek?\n              metrics={\"acc\": accuracy},          # optional, you can not specify any metrics at all\n              opt=torch.optim.Adam,               # optimizer class. if note specifiyng, \n                                                  # an SGD is using by default\n              opt_params={\"weight_decay\": 1e-5})  # optimizer kwargs in dict format (optional too)\n\n# Actually, there are a lot of params for kekers, but this out of scope of this example\n# you can read about them in Keker's docstring (but who really reads the docs, huh?)"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# before the start of the finetuning procedure let's freeeze all the layers except the last one - the head\n# the `freeze` method is mostly inspired (or stolen) from fastai\n# but you should define a model's attribute to deal with\n# for example, our model is actually model.net, so we need to specify the 'net' attr\n# also this method does not freezes batchnorm layers by default. To change this set `freeze_bn=True`\nkeker.freeze(model_attr=\"net\")"},{"cell_type":"markdown","metadata":{},"source":["## Logdir"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"def get_logdir(root=\"logdir\"):\n    root = Path(root)\n    root.mkdir(exist_ok=True)\n    basename = \"kek\"\n    i = 0\n    while (root / f\"kek_{i}\").is_dir():\n        i += 1\n    return root / f\"kek_{i}\""},{"cell_type":"markdown","metadata":{},"source":["## Learning Rate Find"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# let's find an 'optimal' learning rate with learning rate find procedure\n# for details please see the fastai course and this articles:\n# https://arxiv.org/abs/1803.09820\n# https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html\n\n# NOTE: this is an optional step and you can skip it and use your favorite learning rate\n\n# you MUST specify the logdir to see graphics\n# keker will write a tensorboard logs into this folder\n# to see them start a tensorboard with `--logdir /path/to/logdir`\n# OR you can use keker.plot_kek_lr method (see cell below)\nkek_lr_logdir = get_logdir()\nkeker.kek_lr(final_lr=0.1, logdir=kek_lr_logdir)"},{"cell_type":"markdown","metadata":{},"source":"## Plot Learning Rate find results"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"enable_plotly_in_cell()  # only for google colab\n\n# Zoom in plot to see on which step the loss was still decreasing\n# and choose LR from this step\nkeker.plot_kek_lr(logdir=kek_lr_logdir)"},{"cell_type":"markdown","metadata":{},"source":["## Simple Kek"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# Ok, now let's start training!\n# It's as simple as:\nkeker.kek(lr=1e-3, epochs=3)"},{"cell_type":"markdown","metadata":{},"source":["## Kek with different optimizer"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# SomeKekasUser: Wait, and what if I want to train with the different optimizer?\n#\n# Me:\nkeker.kek(lr=1e-3, \n          epochs=1,\n          opt=torch.optim.RMSprop,            # optimizer class\n          opt_params={\"weight_decay\": 1e-5})  # optimizer kwargs in dict format (if you want)\n\n# by default, the optimizer specified on Keker initialization is used"},{"cell_type":"markdown","metadata":{},"source":["## Kek with scheduler"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# SomeKekasUser: OK, and what if I want to use a pytorch scheduler?\n#\n# Me:\nkeker.kek(lr=1e-3,\n          epochs=2,\n          sched=torch.optim.lr_scheduler.StepLR,       # pytorch lr scheduler class\n          sched_params={\"step_size\":1, \"gamma\": 0.9})  # schedulres kwargs in dict format\n\n# by default, no scheduler is using"},{"cell_type":"markdown","metadata":{},"source":["## Log your keks"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# SomeKekasUser: How about logging?\n#\n# Me:\nlogdir = get_logdir()\nkeker.kek(lr=1e-3,\n          epochs=5,\n          logdir=logdir)\n\n# It will create a `train` and `val` subfolders in logdir, and will write tensorboard logs into them\n# to see them start a tensorboard with `--logdir /path/to/logdir`\n# OR you can use keker.plot_kek method! (see cell below)"},{"cell_type":"markdown","metadata":{},"source":["## Plot your keks"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"enable_plotly_in_cell()  # only for google colab\n\n# kekas uses plotly lib and tensorboard logs to plot inside NB\nkeker.plot_kek(logdir=logdir,  # path to logdir with logs to plot\n               step=\"epoch\",              # (optional) default is \"batch\". another option is \"epoch\"\n                                          # It determines discreteness of ploting\n               metrics=[\"loss\",           # (optional) list of metrics names\n                        \"acc\"],           # by default [\"loss\", \"lr\"] is using\n                                          # the order of the names determines the order of the plot\n                                          # NOTE: names of metrics must match names in metrics dict\n                                          # which was specified on Keker init step\n               height=1200,               # (optional) height of the total plot \n               width=800)                 # (optional) width of the total plot"},{"cell_type":"markdown","metadata":{},"source":["## Checkpoints saving"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# SomeKekasUser: Also I want to save best checkpoints to later use them for SWA or ensembling!\n#                And I want to measure them by custom metric, control their number, specify their name prefix,\n#                and control what I need - minimize or maximize metric!\n# Me: Here it is:\nkeker.kek(lr=1e-3,\n          epochs=3,\n          cp_saver_params={\n              \"savedir\": get_logdir(),# a directory for checkpoints\n              \"metric\": \"acc\",  # (optional) from `metrics` dict on Keker init. \n                                # default is validation loss\n              \"n_best\": 3,      # (optional) default is 3\n              \"prefix\": \"kek\",  # (optional) default prefix is `checkpoint`\n              \"mode\": \"max\"     # (optional) default is 'min'\n          })   \n\n# It will create a `savedir` directory, and will save best checkpoints there\n# with naming `{prefix}.{epoch_num}.h5`. The best checkpoint will be dublicated with `{prefix}.best.h5` name\n# look at the report down here"},{"cell_type":"markdown","metadata":{},"source":["## Early stopping"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# SomeKekasUser: Allright, and I don't want to train model, if validation loss doesn't improve for several epochs.\n# \n# Me: You mean, early stopping? Here:\nkeker.kek(lr=1e-3,\n          epochs=1, \n          early_stop_params={\n              \"patience\": 3,   # number of bad epochs to wait before stopping\n              \"metric\": \"acc\", # (optional) metric name from 'metric' dict. default is val loss\n              \"mode\": \"min\",   # (optional) what you want from you metric, max or min? default is 'min'\n              \"min_delta\": 0   # (optional) a minimum delta to count an epoch as 'bad'\n          })"},{"cell_type":"markdown","metadata":{},"source":["## Just do it"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# SomeAdvancedKekasUser: I WANT IT ALL!\n# \n# Me: Well, okay then...\nlogdir = get_logdir()\nkeker.kek(lr=1e-3,\n          epochs=5,\n          opt=torch.optim.RMSprop,\n          opt_params={\"weight_decay\": 1e-5},\n          sched=torch.optim.lr_scheduler.StepLR,\n          sched_params={\"step_size\":1, \"gamma\": 0.9},\n          logdir=logdir,\n          cp_saver_params={\n              \"savedir\": logdir,  \n              \"metric\": \"acc\",  \n              \"n_best\": 3,      \n              \"prefix\": \"kek\",  \n              \"mode\": \"max\"},     \n          early_stop_params={\n              \"patience\": 3,   \n              \"metric\": \"acc\", \n              \"mode\": \"min\",   \n              \"min_delta\": 0\n          })"},{"cell_type":"markdown","metadata":{},"source":["## One Cycle Kek!"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# SomeFastaiFan: Did you stole something else from fastai?\n#\n# Me: Yes! One Cycle Policy!\nkeker.kek_one_cycle(max_lr=1e-3,                  # the maximum learning rate\n                    cycle_len=5,                  # number of epochs, actually, but not exactly\n                    momentum_range=(0.95, 0.85),  # range of momentum changes\n                    div_factor=25,                # max_lr / min_lr\n                    increase_fraction=0.3)        # the part of cycle when learning rate increases\n\n# If you don't understand these parameters, read this - https://sgugger.github.io/the-1cycle-policy.html\n# NOTE: you cannot use schedulers and early stopping with one cycle!\n# another options are the same as for `kek` method"},{"cell_type":"markdown","metadata":{},"source":["## Other Keker features"]},{"cell_type":"markdown","metadata":{},"source":["### Freezing / unfreezing"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# We've already talk about freezing. But what if I want to unfreeze?\n# It has the same interface:\nkeker.unfreeze(model_attr=\"net\")\n\n# If you want to freeze till some layer:\nlayer_num = -2\nkeker.freeze_to(layer_num, model_attr=\"net\")"},{"cell_type":"markdown","metadata":{},"source":["### Saving / Loading"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# saving\nkeker.save(logdir / \"kek.h5\")\n\n# loading\nkeker.load(logdir / \"kek.h5\")"},{"cell_type":"markdown","metadata":{},"source":["### Device and DataParallel"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# Keker is using all avialable GPUs by default\n# To limit it, use 'CUDA_VISIBLE_DEVICES' environment variable (available in os.environ dict)\n\n# if you want to specify cuda device for your model, specify `device` parameter on Keker initialization"},{"cell_type":"markdown","metadata":{},"source":["### FP16"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# and that's all (but it's irreversible)\n# it uses apex under the hood\n# you can pass amp params directly to method\nkeker.to_fp16()"},{"cell_type":"markdown","metadata":{},"source":["### Inference"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# there are 4 (yes, four) ways to get a predictions with keker\n\n# 1st\nkeker.predict(savepath=logdir)\n# it will makes predicts on your 'test_dl' dataloader (remember, we initialized it with 'None'), if it specified,\n# and saves models output in numpy.ndarray format to 'savepath'\n\n# 2nd\nloader = val_dl\nkeker.predict_loader(loader=loader, savepath=logdir)\n# it will do the same as `predict()` but on any custom loader you want\n\n# 3rd\ntensor = torch.zeros((4, 3, 224, 224), dtype=torch.float32)\npreds = keker.predict_tensor(tensor=tensor, to_numpy=False)\n# it will return a predictions of the model in numpy format if `'to_numpy==True', else - torch.Tensor\n\n# 4th\narray = np.zeros((4, 3, 224, 224), dtype=np.float32)\npreds = keker.predict_array(array=array, to_numpy=False)\n# it will do the same as `predict_tensor()` but with np.ndarray as input"},{"cell_type":"markdown","metadata":{},"source":["### TTA"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# I am sure that it is not very convinient way for test time augmentations,\n# but here is how you can do it with Kekas\n\n# first, specify several augmentations for TTA\nflip_ = Flip(always_apply=True)\nvertical_flip_ = VerticalFlip(always_apply=True)\ntranspose_ = Transpose(always_apply=True)\n\n# second, create the whole augmentations with theese ones inside\ndef insert_aug(aug, dataset_key=\"image\", size=224):    \n    PRE_TFMS = Transformer(dataset_key, lambda x: cv2.resize(x, (size, size)))\n    \n    AUGS = Transformer(dataset_key, lambda x: aug(image=x)[\"image\"])\n    \n    NRM_TFMS = transforms.Compose([\n        Transformer(dataset_key, to_torch()),\n        Transformer(dataset_key, normalize())\n    ])\n    \n    tfm = transforms.Compose([PRE_TFMS, AUGS, NRM_TFMS])\n    return tfm\n\n\nflip = insert_aug(flip_)\nvertical_flip = insert_aug(vertical_flip_)\ntranspose = insert_aug(transpose_)\n\ntta_tfms = {\"flip\": flip, \"v_flip\": vertical_flip, \"transpose\": transpose}\n\n# third, run TTA\nkeker.TTA(loader=val_dl,                # loader to predict on \n          tfms=tta_tfms,                # list or dict of always applying transforms\n          savedir=logdir,               # savedir\n          prefix=\"preds\")               # (optional) name prefix. default is 'preds'\n\n# it will saves predicts for each augmentation to savedir with name\n#  - {prefix}_{name_from_dict}.npy if tfms is a dict\n#  - {prefix}_{index}.npy          if tfms is a list"},{"cell_type":"markdown","metadata":{},"source":["# Callbacks"]},{"cell_type":"markdown","metadata":{},"source":["## Adding callbacks"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# Callbacks is the way in which Kekas customizes its pipeline\n# each callback implements six methods, which names tell when it applies\n# on_train_begin()\n#     on_epoch_begin()\n#         on_batch_begin()\n#             >>>... step here ...<<<\n#         on_batch_end()\n#     on_epoch_end()\n# on_train_end()\n\n# Callbacks are widely using under the hood of Kekas\n# For example - loss, opimizer, progressbar, lr scheduling, checkpoint saving, early stopping etc\n# are realized as callbacks\n\n# Callback has access to `state` attr of a keker. Here is a docs from Keker about state:\n\n        # The state is an object that stores many variables and represents\n        # the state of your train-val-repdict pipeline. _state passed to every\n        # callback call.\n        # You can use it as a container for your custom variables, but\n        # DO NOT USE the following ones:\n        #\n        # loss, batch, model, dataowner, criterion, opt, parallel, checkpoint,\n        # stop_iter, stop_epoch, stop_train, out, sched, mode, loader, pbar,\n        # metrics, epoch_metrics\n\n# You can write your own callback, or use something useful from kekas.callbacks\n\n# Callbacks should be passes as a list at the Keker initiation\n# For example, let's use a DebuggerCallback, that just insert a pdb.set_trace() call in pipeline\n# For more info, please see a DebuggerCallback docs and source code\ndebugger = DebuggerCallback(when=[\"on_epoch_begin\"], modes[\"train\"])\n\nkeker = Keker(model=model, dataowner=dataowner, criterion=criterion, callbacks=[debugger])\n\n# also there is a method to add a callbacks to existing Keker\n\nkeker.add_callbacks([debugger])"},{"cell_type":"markdown","metadata":{},"source":["## Custom loss and opimizer callbacks"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# As was said, loss and optimezer behavior is realiesed as Callbacks.\n# If you use some tricky loss or optimizer logic, you can create your own Callback\n# and specify it during Keker initialization\n\n# here are the callbacks, that are using by default\nclass LossCallback(Callback):\n    def __init__(self, target_key, preds_key) -> None:\n        # target_key and preds_key are the parameters of Keker\n        self.target_key = target_key\n        self.preds_key = preds_key\n\n    def on_batch_end(self, i, state) -> None:\n        target = state.core.batch[self.target_key]\n        preds = state.core.out[self.preds_key]\n\n        state.core.loss = state.core.criterion(preds, target)\n\nclass OptimizerCallback(Callback):\n    def on_batch_end(self, i, state) -> None:\n        if state.core.mode == \"train\":\n            state.core.opt.zero_grad()\n            if state.core.use_fp16:\n                with amp.scale_loss(state.core.loss, state.core.opt) as scaled_loss:\n                    scaled_loss.backward()\n            else:\n                state.core.loss.backward()\n            state.core.opt.step()\n            \n# and here is how you should specify them during Keker initialization\nkeker = Keker(model=model, \n              dataowner=dataowner,\n              criterion=criterion,\n              loss_cb=LossCallback,\n              opt_cb=OptimizerCallback)"},{"cell_type":"markdown","metadata":{},"source":["# Notes"]},{"cell_type":"markdown","metadata":{},"source":["I hope you now got an idea how to use Kekas.\n","\n","I will be happy to get feedback about my library and this tutorial.\n","\n","You can find me in [OpenDataScience](http://ods.ai) community by @belskikh nikname or create an issue on GitHub.\n","\n","Have a good keks!"]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}