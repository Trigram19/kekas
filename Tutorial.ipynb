{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdb import set_trace as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pretrainedmodels as pm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from albumentations import Compose, JpegCompression, CLAHE, RandomRotate90, Transpose, ShiftScaleRotate, \\\n",
    "        Blur, OpticalDistortion, GridDistortion, HueSaturationValue, Flip, VerticalFlip\n",
    "\n",
    "from kekas import Keker, DataOwner, DataKek\n",
    "from kekas.transformations import Transformer, to_torch, normalize\n",
    "from kekas.metrics import accuracy\n",
    "from kekas.modules import Flatten, AdaptiveConcatPool2d\n",
    "from kekas.callbacks import Callback, Callbacks, DebuggerCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how to build a classification pipeline with Kekas.\n",
    "We will finetune a convolutional neural network on Cats and Dogs dataset.\n",
    "\n",
    "Firstly, download Cats and Dogs dataset from https://www.microsoft.com/en-us/download/details.aspx?id=54765 and unpack it wherever you want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe creation and train/val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fpath</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PetImages/Dog/3208.jpg</td>\n",
       "      <td>Dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PetImages/Dog/8355.jpg</td>\n",
       "      <td>Dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PetImages/Dog/7032.jpg</td>\n",
       "      <td>Dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PetImages/Dog/872.jpg</td>\n",
       "      <td>Dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PetImages/Dog/1438.jpg</td>\n",
       "      <td>Dog</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    fpath label\n",
       "0  PetImages/Dog/3208.jpg   Dog\n",
       "1  PetImages/Dog/8355.jpg   Dog\n",
       "2  PetImages/Dog/7032.jpg   Dog\n",
       "3   PetImages/Dog/872.jpg   Dog\n",
       "4  PetImages/Dog/1438.jpg   Dog"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's create a pandas DataFrame to help us with data handling\n",
    "root_dir = Path(\"PetImages/\")  # path to Cats and Dogs dataset root directory\n",
    "\n",
    "fpaths = []\n",
    "labels = []\n",
    "for d in root_dir.iterdir():\n",
    "    for f in d.iterdir():\n",
    "        img = cv2.imread(str(f))  # some files there are corrupted, so add only good ones\n",
    "        if img is not None:\n",
    "            labels.append(d.name)\n",
    "            fpaths.append(str(f))\n",
    "\n",
    "df = pd.DataFrame(data={\"fpath\": fpaths, \"label\": labels})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((22946, 2), (2000, 2))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split dataset to train and val parts\n",
    "train_df, val_df = train_test_split(df, test_size=2000)\n",
    "train_df.shape, val_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train and val datasets using DataKek class - a pytorch Dataset that uses pandas DataFrame as data source\n",
    "\n",
    "# at first we need to create a reader function that will define how image will be opened\n",
    "def reader_fn(i, row):\n",
    "    # it always gets i and row as parameters\n",
    "    # where i is an index of dataframe and row is a dataframes row\n",
    "    image = cv2.imread(row[\"fpath\"])[:,:,::-1]  # BGR -> RGB\n",
    "    if row[\"label\"] == \"Dog\":\n",
    "        label = 0\n",
    "    else:\n",
    "        label = 1\n",
    "    return {\"image\": image, \"label\": label}\n",
    "\n",
    "\n",
    "# Then we should create transformations/augmentations\n",
    "# We will use awesome https://github.com/albu/albumentations library\n",
    "def augs(p=0.5):\n",
    "    return Compose([\n",
    "        CLAHE(),\n",
    "        RandomRotate90(),\n",
    "        Transpose(),\n",
    "        ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.50, rotate_limit=45, p=.75),\n",
    "        Blur(blur_limit=3),\n",
    "        OpticalDistortion(),\n",
    "        GridDistortion(),\n",
    "        HueSaturationValue()\n",
    "    ], p=p)\n",
    "\n",
    "def get_transforms(dataset_key, size, p):\n",
    "    # we need to use a Transformer class to apply transformations to DataKeks elements\n",
    "    # dataset_key is an image key in dict returned by reader_fn\n",
    "    \n",
    "    PRE_TFMS = Transformer(dataset_key, lambda x: cv2.resize(x, (size, size)))\n",
    "\n",
    "    AUGS = Transformer(dataset_key, lambda x: augs()(image=x)[\"image\"])\n",
    "\n",
    "    NRM_TFMS = transforms.Compose([\n",
    "        Transformer(dataset_key, to_torch()),\n",
    "        Transformer(dataset_key, normalize())\n",
    "    ])\n",
    "    \n",
    "    train_tfms = transforms.Compose([PRE_TFMS, AUGS, NRM_TFMS])\n",
    "    val_tfms = transforms.Compose([PRE_TFMS, NRM_TFMS])  # because we don't want to augment val set yet\n",
    "    \n",
    "    return train_tfms, val_tfms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataKeks creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's create DataKeks\n",
    "train_tfms, val_tfms = get_transforms(\"image\", 224, 0.5)\n",
    "\n",
    "train_dk = DataKek(df=train_df, reader_fn=reader_fn, transforms=train_tfms)\n",
    "val_dk = DataKek(df=val_df, reader_fn=reader_fn, transforms=val_tfms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and DataLoaders\n",
    "batch_size = 32\n",
    "workers = 8\n",
    "\n",
    "train_dl = DataLoader(train_dk, batch_size=batch_size, num_workers=workers, shuffle=True, drop_last=True)\n",
    "val_dl = DataLoader(val_dk, batch_size=batch_size, num_workers=workers, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a simple neural network using pretrainedmodels library\n",
    "# https://github.com/Cadene/pretrained-models.pytorch\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_classes: int,\n",
    "            p: float = 0.5,\n",
    "            pooling_size: int = 2,\n",
    "            last_conv_size: int = 2048,\n",
    "            arch: str = \"se_resnext50_32x4d\",\n",
    "            pretrained: str = \"imagenet\") -> None:\n",
    "        \"\"\"A simple model to finetune.\n",
    "        \n",
    "        Args:\n",
    "            num_classes: the number of target classes, the size of the last layer's output\n",
    "            p: dropout probability\n",
    "            pooling_size: the size of the result feature map after adaptive pooling layer\n",
    "            last_conv_size: size of the flatten last backbone conv layer\n",
    "            arch: the name of the architecture form pretrainedmodels\n",
    "            pretrained: the mode for pretrained model from pretrainedmodels\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        net = pm.__dict__[arch](pretrained=pretrained)\n",
    "        modules = list(net.children())[:-2]  # delete last layers: pooling and linear\n",
    "        \n",
    "        # add custom head\n",
    "        modules += [nn.Sequential(\n",
    "            # AdaptiveConcatPool2d is a concat of AdaptiveMaxPooling and AdaptiveAveragePooling \n",
    "            AdaptiveConcatPool2d(size=pooling_size),\n",
    "            Flatten(),\n",
    "            nn.BatchNorm1d(2 * pooling_size * pooling_size * last_conv_size),\n",
    "            nn.Dropout(p),\n",
    "            nn.Linear(2 * pooling_size * pooling_size * last_conv_size, num_classes)\n",
    "        )]\n",
    "        self.net = nn.Sequential(*modules)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.net(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the three whales of your pipelane are: the data, the model and the loss (hi, Jeremy)\n",
    "\n",
    "# the data is represented in Kekas by DataOwner. It is a namedtuple with three fields:\n",
    "# 'train_dl', 'val_dl', 'test_dl'\n",
    "# For training process we will need at least two of them, and we can skip 'test_dl' for now\n",
    "# so we will initialize it with `None` value.\n",
    "dataowner = DataOwner(train_dl, val_dl, None)\n",
    "\n",
    "# model is just a pytorch nn.Module, that we created vefore\n",
    "model = Net(num_classes=2)\n",
    "\n",
    "# loss or criterion is also a pytorch nn.Module. For multiloss scenarios it can be a list of nn.Modules\n",
    "# for our simple example let's use the standart cross entopy criterion\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also we need to specify, what model will do with each batch of data on each iteration\n",
    "# We should define a `step_fn` function\n",
    "# The code below repeats a `keker.default_step_fn` code to provide you with a concept of step function\n",
    "\n",
    "def step_fn(model: torch.nn.Module,\n",
    "            batch: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Determine what your model will do with your data.\n",
    "\n",
    "    Args:\n",
    "        model: the pytorch module to pass input in\n",
    "        batch: the batch of data from the DataLoader\n",
    "\n",
    "    Returns:\n",
    "        The models forward pass results\n",
    "    \"\"\"\n",
    "    \n",
    "    # you could define here whatever logic you want\n",
    "    inp = batch[\"image\"]  # here we get an \"image\" from our dataset\n",
    "    return model(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# previous preparations was mostly out of scope of Kekas library (except DataKeks creation)\n",
    "# Now let's dive into kekas a little bit\n",
    "\n",
    "# firstly, we create a Keker - the core Kekas class, that provides all the keks for your pipeline\n",
    "keker = Keker(model=model,\n",
    "              dataowner=dataowner,\n",
    "              criterion=criterion,\n",
    "              step_fn=step_fn,                    # previosly defined step function\n",
    "              target_key=\"label\",                 # remember, we defined it in the reader_fn for DataKek?\n",
    "              metrics={\"acc\": accuracy},          # optional, you can not specify any metrics at all\n",
    "              opt=torch.optim.Adam,               # optimizer class. if note specifiyng, \n",
    "                                                  # an SGD is using by default\n",
    "              opt_params={\"weight_decay\": 1e-5})  # optimizer kwargs in dict format (optional too)\n",
    "\n",
    "# Actually, there are a lot of params for kekers, but this out of scope of this example\n",
    "# you can read about them in Keker's docstring (but who really reads the docs, huh?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before the start of the finetuning procedure let's freeeze all the layers except the last one - the head\n",
    "# the `freeze` method is mostly inspired (or stolen) from fastai\n",
    "# but you should define a model's attribute to deal with\n",
    "# for example, our model is actually model.net, so we need to specify the 'net' attr\n",
    "# also this method does not freezes batchnorm layers by default. To change this set `freeze_bn=True`\n",
    "keker.unfreeze(model_attr=\"net\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate Find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100% 717/717 [02:41<00:00,  4.44it/s, loss=101.3424]\n",
      "End of LRFinder\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's find an 'optimal' learning rate with learning rate find procedure\n",
    "# for details please see the fastai course and this articles:\n",
    "# https://arxiv.org/abs/1803.09820\n",
    "# https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html\n",
    "\n",
    "# NOTE: this is an optional step and you can skip it and use your favorite learning rate\n",
    "\n",
    "# you MUST specify the logdir to see graphics\n",
    "# keker will write a tensorboard logs into this folder\n",
    "# to see them start a tensorboard with `--logdir /path/to/logdir`\n",
    "# OR you can use keker.plot_kek_lr method (see cell below)\n",
    "keker.kek_lr(final_lr=0.1, logdir=\"/path/to/logdir\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Learning Rate find results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zoom in plot to see on which step the loss was still decreasing\n",
    "# and choose LR from this step\n",
    "keker.plot_kek_lr(logdir=\"/path/to/logdir\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Kek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100% 717/717 [02:17<00:00,  6.00it/s, loss=0.1613, val_loss=0.0292, acc=0.9881]\n",
      "Epoch 2/3: 100% 717/717 [02:16<00:00,  5.35it/s, loss=0.1650, val_loss=0.0292, acc=0.9891]\n",
      "Epoch 3/3: 100% 717/717 [02:16<00:00,  5.21it/s, loss=0.1229, val_loss=0.0259, acc=0.9906]\n"
     ]
    }
   ],
   "source": [
    "# Ok, now let's start training!\n",
    "# It's as simple as:\n",
    "keker.kek(lr=1e-5, epochs=3)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kek with different optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100% 717/717 [02:15<00:00,  5.42it/s, loss=0.1829, val_loss=0.0387, acc=0.9916]\n"
     ]
    }
   ],
   "source": [
    "# SomeKekasUser: Wait, and what if I want to train with the different optimizer?\n",
    "#\n",
    "# Me:\n",
    "keker.kek(lr=1e-5, \n",
    "          epochs=1,\n",
    "          opt=torch.optim.RMSprop,            # optimizer class\n",
    "          opt_params={\"weight_decay\": 1e-5})  # optimizer kwargs in dict format (if you want)\n",
    "\n",
    "# by default, the optimizer specified on Keker initialization is used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kek with scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100% 717/717 [02:16<00:00,  6.00it/s, loss=0.1569, val_loss=0.0298, acc=0.9876]\n",
      "Epoch 2/2: 100% 717/717 [02:17<00:00,  5.98it/s, loss=0.1267, val_loss=0.0272, acc=0.9881]\n"
     ]
    }
   ],
   "source": [
    "# SomeKekasUser: OK, and what if I want to use a pytorch scheduler?\n",
    "#\n",
    "# Me:\n",
    "keker.kek(lr=1e-5,\n",
    "          epochs=2,\n",
    "          sched=torch.optim.lr_scheduler.StepLR,       # pytorch lr scheduler class\n",
    "          sched_params={\"step_size\":1, \"gamma\": 0.9})  # schedulres kwargas in dict format\n",
    "\n",
    "# by default, no scheduler is using"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log your keks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100% 717/717 [02:47<00:00,  4.43it/s, loss=0.0520, val_loss=0.0258, acc=0.9911]\n"
     ]
    }
   ],
   "source": [
    "# SomeKekasUser: How about logging?\n",
    "#\n",
    "# Me:\n",
    "keker.kek(lr=1e-5,\n",
    "          epochs=1,\n",
    "          logdir=\"/mnt/hdd3_4/belskikh/keks/forplot\")\n",
    "\n",
    "# It will create a `train` and `val` subfolders in logdir, and will write tensorboard logs into them\n",
    "# to see them start a tensorboard with `--logdir /path/to/logdir`\n",
    "# OR you can use keker.plot_kek method! (see cell below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot your keks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kekas uses plotly lib and tensorboard logs to plot inside NB\n",
    "keker.plot_kek(logdir=\"/path/to/logdir\",  # path to logdir with logs to plot\n",
    "               step=\"batch\",              # (optional) default is \"step\". another option is \"epoch\"\n",
    "                                          # It determines discreteness of ploting\n",
    "               metrics=[\"loss\",           # (optional) list of metrics names\n",
    "                        \"acc\",            # by default [\"loss\", \"lr\"] is using\n",
    "                        \"lr\"],            # the order of the names determines the order of the plot\n",
    "                                          # NOTE: names of metrics must match names in metrics dict\n",
    "                                          # which was specified on Keker init step\n",
    "               height=1200,               # (optional) height of the total plot \n",
    "               width=800)                 # (optional) width of the total plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoints saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100% 717/717 [02:16<00:00,  5.75it/s, loss=0.1404, val_loss=0.0356, acc=0.9901]\n",
      "\n",
      "Checkpoint\tacc\n",
      "/path/to/save/dir/kek.1.h5\t0.990079\n"
     ]
    }
   ],
   "source": [
    "# SomeKekasUser: Also I want to save best checkpoints to later use them for SWA or ensembling!\n",
    "#                And I want to measure them by custom metric, control their number, specify their name prefix,\n",
    "#                and control what I need - minimize or maximize metric!\n",
    "# Me: Here it is:\n",
    "keker.kek(lr=1e-5,\n",
    "          epochs=1,\n",
    "          cp_saver_params={\n",
    "              \"savedir\": \"/path/to/save/dir\",  # a directory for checkpoints\n",
    "              \"metric\": \"acc\",  # (optional) from `metrics` dict on Keker init. \n",
    "                                # default is validation loss\n",
    "              \"n_best\": 3,      # (optional) default is 3\n",
    "              \"prefix\": \"kek\",  # (optional) default prefix is `checkpoint`\n",
    "              \"mode\": \"max\"     # (optional) default is 'min'\n",
    "          })   \n",
    "\n",
    "# It will create a `savedir` directory, and will save best checkpoints there\n",
    "# with naming `{prefix}.{epoch_num}.h5`. The best checkpoint will be dublicated with `{prefix}.best.h5` name\n",
    "# look at the report down here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100% 717/717 [02:17<00:00,  5.78it/s, loss=0.1577, val_loss=0.0341, acc=0.9901]\n"
     ]
    }
   ],
   "source": [
    "# SomeKekasUser: Allright, and I don't want to train model, if validation loss doesn't improve for several epochs.\n",
    "# \n",
    "# Me: You mean, early stopping? Here:\n",
    "keker.kek(lr=1e-5,\n",
    "          epochs=1, \n",
    "          early_stop_params={\n",
    "              \"patience\": 3,   # number of bad epochs to wait before stopping\n",
    "              \"metric\": \"acc\", # (optional) metric name from 'metric' dict. default is val loss\n",
    "              \"mode\": \"min\",   # (optional) what you want from you metric, max or min? default is 'min'\n",
    "              \"min_delta\": 0   # (optional) a minimum delta to count an epoch as 'bad'\n",
    "          })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just do it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100% 717/717 [02:16<00:00,  5.96it/s, loss=0.1697, val_loss=0.0276, acc=0.9926]\n",
      "Epoch 2/5: 100% 717/717 [02:16<00:00,  5.11it/s, loss=0.1241, val_loss=0.0321, acc=0.9901]\n",
      "Epoch 3/5: 100% 717/717 [02:16<00:00,  5.97it/s, loss=0.1576, val_loss=0.0338, acc=0.9901]\n",
      "Epoch 4/5: 100% 717/717 [02:16<00:00,  5.99it/s, loss=0.1733, val_loss=0.0300, acc=0.9906]\n",
      "Epoch 5/5: 100% 717/717 [02:17<00:00,  5.94it/s, loss=0.1397, val_loss=0.0310, acc=0.9916]\n",
      "\n",
      "Checkpoint\tacc\n",
      "/path/to/save/dir/kek.1.h5\t0.992560\n",
      "/path/to/save/dir/kek.5.h5\t0.991567\n",
      "/path/to/save/dir/kek.4.h5\t0.990575\n"
     ]
    }
   ],
   "source": [
    "# SomeAdvancedKekasUser: I WANT IT ALL!\n",
    "# \n",
    "# Me: Well, okay then...\n",
    "keker.kek(lr=1e-5,\n",
    "          epochs=5,\n",
    "          opt=torch.optim.RMSprop,\n",
    "          opt_params={\"weight_decay\": 1e-5},\n",
    "          sched=torch.optim.lr_scheduler.StepLR,\n",
    "          sched_params={\"step_size\":1, \"gamma\": 0.9},\n",
    "          logdir=\"/path/to/logdir\",\n",
    "          cp_saver_params={\n",
    "              \"savedir\": \"/path/to/save/dir\",  \n",
    "              \"metric\": \"acc\",  \n",
    "              \"n_best\": 3,      \n",
    "              \"prefix\": \"kek\",  \n",
    "              \"mode\": \"max\"},     \n",
    "          early_stop_params={\n",
    "              \"patience\": 3,   \n",
    "              \"metric\": \"acc\", \n",
    "              \"mode\": \"min\",   \n",
    "              \"min_delta\": 0\n",
    "          })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Cycle Kek!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100% 717/717 [02:16<00:00,  5.58it/s, loss=0.1120, val_loss=0.0294, acc=0.9906]\n",
      "Epoch 2/5: 100% 717/717 [02:17<00:00,  5.94it/s, loss=0.1517, val_loss=0.0305, acc=0.9906]\n",
      "Epoch 3/5: 100% 717/717 [02:16<00:00,  5.75it/s, loss=0.1507, val_loss=0.0307, acc=0.9901]\n",
      "Epoch 4/5: 100% 717/717 [02:16<00:00,  5.02it/s, loss=0.1187, val_loss=0.0336, acc=0.9906]\n",
      "Epoch 5/5: 100% 717/717 [02:16<00:00,  5.03it/s, loss=0.0963, val_loss=0.0270, acc=0.9911]\n"
     ]
    }
   ],
   "source": [
    "# SomeFastaiFan: Did you stole something else from fastai?\n",
    "#\n",
    "# Me: Yes! One Cycle Policy!\n",
    "keker.kek_one_cycle(max_lr=1e-5,                  # the maximum learning rate\n",
    "                    cycle_len=5,                  # number of epochs, actually, but not exactly\n",
    "                    momentum_range=(0.95, 0.85),  # range of momentum changes\n",
    "                    div_factor=25,                # max_lr / min_lr\n",
    "                    increase_fraction=0.3)        # the part of cycle when learning rate increases\n",
    "\n",
    "# If you don't understand these parameters, read this - https://sgugger.github.io/the-1cycle-policy.html\n",
    "# NOTE: you cannot use schedulers and early stopping with one cycle!\n",
    "# another options are the same as for `kek` method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Keker features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freezing / unfreezing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We've already talk about freezing. But what if I want to unfreeze?\n",
    "# It has the same interface:\n",
    "keker.unfreeze(model_attr=\"net\")\n",
    "\n",
    "# If you want to freeze till some layer:\n",
    "layer_num = -2\n",
    "keker.freeze_to(layer_num, model_attr=\"net\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving / Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving\n",
    "keker.save(\"/path/to/file\")\n",
    "\n",
    "# loading\n",
    "keker.load(\"/path/to/file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device and DataParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keker is using all avialable GPUs by default\n",
    "# To limit it, use 'CUDA_VISIBLE_DEVICES' environment variable (available in os.environ dict)\n",
    "\n",
    "# if you want to specify cuda device for your model, specify `device` parameter on Keker initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are 4 (yes, four) ways to get a predictions with keker\n",
    "\n",
    "# 1st\n",
    "keker.predict(savepath=\"/path/to/save/dir\")\n",
    "# it will makes predicts on your 'test_dl' dataloader (remember, we initialized it with 'None'), if it specified,\n",
    "# and saves models output in numpy.ndarray format to 'savepath'\n",
    "\n",
    "# 2nd\n",
    "loader = val_dl\n",
    "keker.predict_loader(loader=loader, savepath=\"/path/to/save/dir\")\n",
    "# it will do the same as `predict()` but on any custom loader you want\n",
    "\n",
    "# 3rd\n",
    "tensor = torch.zeros(4, 224, 224, 3)\n",
    "preds = keker.predict_tensor(tensor=tensor, to_numpy=False)\n",
    "# it will return a predictions of the model in numpy format if `'to_numpy==True', else - torch.Tensor\n",
    "\n",
    "# 4th\n",
    "array = np.zeros((4, 224, 224, 3))\n",
    "preds = keker.predict_array(array=array, to_numpy=False)\n",
    "# it will do the same as `predict_tensor()` but with np.ndarra as input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict: 100% 63/63 [00:04<00:00, 15.09it/s]\n",
      "Predict: 100% 63/63 [00:05<00:00, 14.98it/s]\n",
      "Predict: 100% 63/63 [00:05<00:00, 15.01it/s]\n"
     ]
    }
   ],
   "source": [
    "# I am sure that it is not very convinient way for test time augmentations,\n",
    "# but here is how you can do it with Kekas\n",
    "\n",
    "# first, specify several augmentations for TTA\n",
    "flip_ = Flip(always_apply=True)\n",
    "vertical_flip_ = VerticalFlip(always_apply=True)\n",
    "transpose_ = Transpose(always_apply=True)\n",
    "\n",
    "# second, create the whole augmentations with theese ones inside\n",
    "def insert_aug(aug, dataset_key=\"image\", size=224):    \n",
    "    PRE_TFMS = Transformer(dataset_key, lambda x: cv2.resize(x, (size, size)))\n",
    "    \n",
    "    AUGS = Transformer(dataset_key, lambda x: aug(image=x)[\"image\"])\n",
    "    \n",
    "    NRM_TFMS = transforms.Compose([\n",
    "        Transformer(dataset_key, to_torch()),\n",
    "        Transformer(dataset_key, normalize())\n",
    "    ])\n",
    "    \n",
    "    tfm = transforms.Compose([PRE_TFMS, AUGS, NRM_TFMS])\n",
    "    return tfm\n",
    "\n",
    "\n",
    "flip = insert_aug(flip_)\n",
    "vertical_flip = insert_aug(vertical_flip_)\n",
    "transpose = insert_aug(transpose_)\n",
    "\n",
    "tta_tfms = {\"flip\": flip, \"v_flip\": vertical_flip, \"transpose\": transpose}\n",
    "\n",
    "# third, run TTA\n",
    "keker.TTA(loader=val_dl,                # loader to predict on \n",
    "          tfms=tta_tfms,                # list or dict of always applying transforms\n",
    "          savedir=\"/path/to/save/dir\",  # savedir\n",
    "          prefix=\"preds\")               # (optional) name prefix. default is 'preds'\n",
    "\n",
    "# it will saves predicts for each augmentation to savedir with name\n",
    "#  - {prefix}_{name_from_dict}.npy if tfms is a dict\n",
    "#  - {prefix}_{index}.npy          if tfms is a list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks is the way in which Kekas customizes its pipeline\n",
    "# each callback implements six methods, which names tell when it applies\n",
    "# on_train_begin()\n",
    "#     on_epoch_begin()\n",
    "#         on_batch_begin()\n",
    "#             >>>... step here ...<<<\n",
    "#         on_batch_end()\n",
    "#     on_epoch_end()\n",
    "# on_train_end()\n",
    "\n",
    "# Callbacks are widely using under the hood of Kekas\n",
    "# For example - loss, opimizer, progressbar, lr scheduling, checkpoint saving, early stopping etc\n",
    "# are realized as callbacks\n",
    "\n",
    "# Callback has access to `state` attr of a keker. Here is a docs from Keker about state:\n",
    "\n",
    "        # The state is an object that stores many variables and represents\n",
    "        # the state of your train-val-repdict pipeline. _state passed to every\n",
    "        # callback call.\n",
    "        # You can use it as a container for your custom variables, but\n",
    "        # DO NOT USE the following ones:\n",
    "        #\n",
    "        # loss, batch, model, dataowner, criterion, opt, parallel, checkpoint,\n",
    "        # stop_iter, stop_epoch, stop_train, out, sched, mode, loader, pbar,\n",
    "        # metrics, epoch_metrics\n",
    "\n",
    "# You can write your own callback, or use something useful from kekas.callbacks\n",
    "\n",
    "# Callbacks should be passes as a list at the Keker initiation\n",
    "# For example, let's use a DebuggerCallback, that just insert a pdb.set_trace() call in pipeline\n",
    "# For more info, please see a DebuggerCallback docs and source code\n",
    "debugger = DebuggerCallback(when=[\"on_epoch_begin\"], modes[\"train\"])\n",
    "\n",
    "keker = Keker(model=model, dataowner=dataowner, criterion=criterion, callbacks=[debugger])\n",
    "\n",
    "# also there is a method to add a callbacks to existing Keker\n",
    "\n",
    "keker.add_callbacks([debugger])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom loss and opimizer callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As was said, loss and optimezer behavior is realiesed as Callbacks.\n",
    "# If you use some tricky loss or optimizer logic, you can create your own Callback\n",
    "# and specify it during Keker initialization\n",
    "\n",
    "# here are the callbacks, that are using by default\n",
    "class LossCallback(Callback):\n",
    "    def __init__(self, target_key: str, preds_key: str) -> None:\n",
    "        # target_key and preds_key are the parameters of Keker\n",
    "        self.target_key = target_key\n",
    "        self.preds_key = preds_key\n",
    "\n",
    "    def on_batch_end(self, i: int, state: DotDict) -> None:\n",
    "        target = state.batch[self.target_key]\n",
    "        preds = state.out[self.preds_key]\n",
    "\n",
    "        state.loss = state.criterion(preds, target)\n",
    "\n",
    "class OptimizerCallback(Callback):\n",
    "    def on_batch_end(self, i: int, state: DotDict) -> None:\n",
    "        if state.mode == \"train\":\n",
    "            state.opt.zero_grad()\n",
    "            state.loss.backward()\n",
    "            state.opt.step()\n",
    "            \n",
    "# and here is how you should specify them during Keker initialization\n",
    "keker = Keker(model=model, \n",
    "              dataowner=dataowner,\n",
    "              criterion=criterion,\n",
    "              loss_cb=LossCallback,\n",
    "              opt_cb=OptimizerCallback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I hope you now got an idea how to use Kekas.\n",
    "\n",
    "I will be happy to get feedback about my library and this tutorial.\n",
    "\n",
    "You can find me in [OpenDataScience](http://ods.ai) community by @belskikh nikname or create an issue on GitHub.\n",
    "\n",
    "Have a good keks!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
