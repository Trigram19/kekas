{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdb import set_trace as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pretrainedmodels as pm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from albumentations import Compose, JpegCompression, CLAHE, RandomRotate90, Transpose, ShiftScaleRotate, \\\n",
    "        Blur, OpticalDistortion, GridDistortion, HueSaturationValue, Flip, VerticalFlip\n",
    "\n",
    "from kekas import Keker, DataOwner, DataKek\n",
    "from kekas.transformations import Transformer, to_torch, normalize\n",
    "from kekas.metrics import accuracy\n",
    "from kekas.modules import Flatten, AdaptiveConcatPool2d\n",
    "from kekas.callbacks import Callback, Callbacks, DebuggerCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how to build a classification pipeline with Kekas.\n",
    "We will finetune a convolutional neural network on Cats and Dogs dataset.\n",
    "\n",
    "Firstly, download Cats and Dogs dataset from https://www.microsoft.com/en-us/download/details.aspx?id=54765 and unpack it wherever you want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe creation and train/val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fpath</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PetImages/Dog/3208.jpg</td>\n",
       "      <td>Dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PetImages/Dog/8355.jpg</td>\n",
       "      <td>Dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PetImages/Dog/7032.jpg</td>\n",
       "      <td>Dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PetImages/Dog/872.jpg</td>\n",
       "      <td>Dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PetImages/Dog/1438.jpg</td>\n",
       "      <td>Dog</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    fpath label\n",
       "0  PetImages/Dog/3208.jpg   Dog\n",
       "1  PetImages/Dog/8355.jpg   Dog\n",
       "2  PetImages/Dog/7032.jpg   Dog\n",
       "3   PetImages/Dog/872.jpg   Dog\n",
       "4  PetImages/Dog/1438.jpg   Dog"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's create a pandas DataFrame to help us with data handling\n",
    "root_dir = Path(\"PetImages/\")  # path to Cats and Dogs dataset root directory\n",
    "\n",
    "fpaths = []\n",
    "labels = []\n",
    "for d in root_dir.iterdir():\n",
    "    for f in d.iterdir():\n",
    "        img = cv2.imread(str(f))  # some files there are corrupted, so add only good ones\n",
    "        if img is not None:\n",
    "            labels.append(d.name)\n",
    "            fpaths.append(str(f))\n",
    "\n",
    "df = pd.DataFrame(data={\"fpath\": fpaths, \"label\": labels})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((22946, 2), (2000, 2))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split dataset to train and val parts\n",
    "train_df, val_df = train_test_split(df, test_size=2000)\n",
    "train_df.shape, val_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train and val datasets using DataKek class - a pytorch Dataset that uses pandas DataFrame as data source\n",
    "\n",
    "# at first we need to create a reader function that will define how image will be opened\n",
    "def reader_fn(i, row):\n",
    "    # it always gets i and row as parameters\n",
    "    # where i is an index of dataframe and row is a dataframes row\n",
    "    image = cv2.imread(row[\"fpath\"])[:,:,::-1]  # BGR -> RGB\n",
    "    if row[\"label\"] == \"Dog\":\n",
    "        label = 0\n",
    "    else:\n",
    "        label = 1\n",
    "    return {\"image\": image, \"label\": label}\n",
    "\n",
    "\n",
    "# Then we should create transformations/augmentations\n",
    "# We will use awesome https://github.com/albu/albumentations library\n",
    "def augs(p=0.5):\n",
    "    return Compose([\n",
    "        CLAHE(),\n",
    "        RandomRotate90(),\n",
    "        Transpose(),\n",
    "        ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.50, rotate_limit=45, p=.75),\n",
    "        Blur(blur_limit=3),\n",
    "        OpticalDistortion(),\n",
    "        GridDistortion(),\n",
    "        HueSaturationValue()\n",
    "    ], p=p)\n",
    "\n",
    "def get_transforms(dataset_key, size, p):\n",
    "    # we need to use a Transformer class to apply transformations to DataKeks elements\n",
    "    # dataset_key is an image key in dict returned by reader_fn\n",
    "    \n",
    "    PRE_TFMS = Transformer(dataset_key, lambda x: cv2.resize(x, (size, size)))\n",
    "\n",
    "    AUGS = Transformer(dataset_key, lambda x: augs()(image=x)[\"image\"])\n",
    "\n",
    "    NRM_TFMS = transforms.Compose([\n",
    "        Transformer(dataset_key, to_torch()),\n",
    "        Transformer(dataset_key, normalize())\n",
    "    ])\n",
    "    \n",
    "    train_tfms = transforms.Compose([PRE_TFMS, AUGS, NRM_TFMS])\n",
    "    val_tfms = transforms.Compose([PRE_TFMS, NRM_TFMS])  # because we don't want to augment val set yet\n",
    "    \n",
    "    return train_tfms, val_tfms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataKeks creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's create DataKeks\n",
    "train_tfms, val_tfms = get_transforms(\"image\", 224, 0.5)\n",
    "\n",
    "train_dk = DataKek(df=train_df, reader_fn=reader_fn, transforms=train_tfms)\n",
    "val_dk = DataKek(df=val_df, reader_fn=reader_fn, transforms=val_tfms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and DataLoaders\n",
    "batch_size = 32\n",
    "workers = 8\n",
    "\n",
    "train_dl = DataLoader(train_dk, batch_size=batch_size, num_workers=workers, shuffle=True, drop_last=True)\n",
    "val_dl = DataLoader(val_dk, batch_size=batch_size, num_workers=workers, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a simple neural network using pretrainedmodels library\n",
    "# https://github.com/Cadene/pretrained-models.pytorch\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_classes: int,\n",
    "            p: float = 0.5,\n",
    "            pooling_size: int = 2,\n",
    "            last_conv_size: int = 2048,\n",
    "            arch: str = \"se_resnext50_32x4d\",\n",
    "            pretrained: str = \"imagenet\") -> None:\n",
    "        \"\"\"A simple model to finetune.\n",
    "        \n",
    "        Args:\n",
    "            num_classes: the number of target classes, the size of the last layer's output\n",
    "            p: dropout probability\n",
    "            pooling_size: the size of the result feature map after adaptive pooling layer\n",
    "            last_conv_size: size of the flatten last backbone conv layer\n",
    "            arch: the name of the architecture form pretrainedmodels\n",
    "            pretrained: the mode for pretrained model from pretrainedmodels\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        net = pm.__dict__[arch](pretrained=pretrained)\n",
    "        modules = list(net.children())[:-2]  # delete last layers: pooling and linear\n",
    "        \n",
    "        # add custom head\n",
    "        modules += [nn.Sequential(\n",
    "            # AdaptiveConcatPool2d is a concat of AdaptiveMaxPooling and AdaptiveAveragePooling \n",
    "            AdaptiveConcatPool2d(size=pooling_size),\n",
    "            Flatten(),\n",
    "            nn.BatchNorm1d(2 * pooling_size * pooling_size * last_conv_size),\n",
    "            nn.Dropout(p),\n",
    "            nn.Linear(2 * pooling_size * pooling_size * last_conv_size, num_classes)\n",
    "        )]\n",
    "        self.net = nn.Sequential(*modules)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.net(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the three whales of your pipelane are: the data, the model and the loss (hi, Jeremy)\n",
    "\n",
    "# the data is represented in Kekas by DataOwner. It is a namedtuple with three fields:\n",
    "# 'train_dl', 'val_dl', 'test_dl'\n",
    "# For training process we will need at least two of them, and we can skip 'test_dl' for now\n",
    "# so we will initialize it with `None` value.\n",
    "dataowner = DataOwner(train_dl, val_dl, None)\n",
    "\n",
    "# model is just a pytorch nn.Module, that we created vefore\n",
    "model = Net(num_classes=2)\n",
    "\n",
    "# loss or criterion is also a pytorch nn.Module. For multiloss scenarios it can be a list of nn.Modules\n",
    "# for our simple example let's use the standart cross entopy criterion\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also we need to specify, what model will do with each batch of data on each iteration\n",
    "# We should define a `step_fn` function\n",
    "# The code below repeats a `keker.default_step_fn` code to provide you with a concept of step function\n",
    "\n",
    "def step_fn(model: torch.nn.Module,\n",
    "            batch: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Determine what your model will do with your data.\n",
    "\n",
    "    Args:\n",
    "        model: the pytorch module to pass input in\n",
    "        batch: the batch of data from the DataLoader\n",
    "\n",
    "    Returns:\n",
    "        The models forward pass results\n",
    "    \"\"\"\n",
    "    \n",
    "    # you could define here whatever logic you want\n",
    "    inp = batch[\"image\"]  # here we get an \"image\" from our dataset\n",
    "    return model(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# previous preparations was mostly out of scope of Kekas library (except DataKeks creation)\n",
    "# Now let's dive into kekas a little bit\n",
    "\n",
    "# firstly, we create a Keker - the core Kekas class, that provides all the keks for your pipeline\n",
    "keker = Keker(model=model,\n",
    "              dataowner=dataowner,\n",
    "              criterion=criterion,\n",
    "              step_fn=step_fn,                    # previosly defined step function\n",
    "              target_key=\"label\",                 # remember, we defined it in the reader_fn for DataKek?\n",
    "              metrics={\"acc\": accuracy},          # optional, you can not specify any metrics at all\n",
    "              opt=torch.optim.Adam,               # optimizer class. if note specifiyng, \n",
    "                                                  # an SGD is using by default\n",
    "              opt_params={\"weight_decay\": 1e-5})  # optimizer kwargs in dict format (optional too)\n",
    "\n",
    "# Actually, there are a lot of params for kekers, but this out of scope of this example\n",
    "# you can read about them in Keker's docstring (but who really reads the docs, huh?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before the start of the finetuning procedure let's freeeze all the layers except the last one - the head\n",
    "# the `freeze` method is mostly inspired (or stolen) from fastai\n",
    "# but you should define a model's attribute to deal with\n",
    "# for example, our model is actually model.net, so we need to specify the 'net' attr\n",
    "# also this method does not freezes batchnorm layers by default. To change this set `freeze_bn=True`\n",
    "keker.unfreeze(model_attr=\"net\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate Find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100% 717/717 [02:41<00:00,  4.44it/s, loss=101.3424]\n",
      "End of LRFinder\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's find an 'optimal' learning rate with learning rate find procedure\n",
    "# for details please see the fastai course and this articles:\n",
    "# https://arxiv.org/abs/1803.09820\n",
    "# https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html\n",
    "\n",
    "# NOTE: this is an optional step and you can skip it and use your favorite learning rate\n",
    "\n",
    "# you MUST specify the logdir to see graphics\n",
    "# keker will write a tensorboard logs into this folder\n",
    "# to see them start a tensorboard with `--logdir /path/to/logdir`\n",
    "# OR you can use keker.plot_kek_lr method (see cell below)\n",
    "keker.kek_lr(final_lr=0.1, logdir=\"/path/to/logdir\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Learning Rate find results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div id=\"ed7f06c6-5622-4773-a4a4-a9d6891b3a34\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"ed7f06c6-5622-4773-a4a4-a9d6891b3a34\", [{\"name\": \"train/batch/loss\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716], \"y\": [0.7566803693771362, 0.7663940191268921, 0.7005658745765686, 0.7284559607505798, 0.8431040644645691, 0.8203480839729309, 0.6613660454750061, 0.6647655367851257, 0.7666909694671631, 0.806519091129303, 0.7276999950408936, 0.7613999247550964, 0.7675634622573853, 0.7332559823989868, 0.733258068561554, 0.6708574891090393, 0.8259966969490051, 0.7503928542137146, 0.6988753080368042, 0.7466038465499878, 0.754898726940155, 0.719093382358551, 0.6738682389259338, 0.7057905197143555, 0.7098023295402527, 0.6650568246841431, 0.7581334114074707, 0.7865424752235413, 0.7156451344490051, 0.7176768183708191, 0.7022285461425781, 0.7159243226051331, 0.7310737371444702, 0.6410583257675171, 0.8655389547348022, 0.640640914440155, 0.6897212266921997, 0.728081226348877, 0.6406668424606323, 0.6839047074317932, 0.8045408129692078, 0.6782159805297852, 0.687109112739563, 0.7398470044136047, 0.6881431341171265, 0.6843844056129456, 0.7378577589988708, 0.6784773468971252, 0.6153199672698975, 0.6881710290908813, 0.6925021409988403, 0.6422572135925293, 0.6866356730461121, 0.654623806476593, 0.6337660551071167, 0.6534527540206909, 0.6126432418823242, 0.6040847301483154, 0.7459214329719543, 0.6282187700271606, 0.7168393135070801, 0.7366276383399963, 0.7962068915367126, 0.6437177062034607, 0.7412457466125488, 0.5672217607498169, 0.6640769839286804, 0.6582993865013123, 0.6884856820106506, 0.5784615874290466, 0.5584967136383057, 0.6463703513145447, 0.6808290481567383, 0.6381504535675049, 0.6713171601295471, 0.655731737613678, 0.5494349002838135, 0.6232591867446899, 0.6805684566497803, 0.5239701271057129, 0.6628803610801697, 0.6625112891197205, 0.6549612879753113, 0.5749457478523254, 0.7625686526298523, 0.6639186143875122, 0.5395482182502747, 0.5576159954071045, 0.5888815522193909, 0.5383743047714233, 0.5426575541496277, 0.632208526134491, 0.5313699841499329, 0.52064448595047, 0.6755347847938538, 0.5635552406311035, 0.5732156038284302, 0.7404903173446655, 0.5726014375686646, 0.5399105548858643, 0.48364946246147156, 0.6058472394943237, 0.48406079411506653, 0.4989832639694214, 0.47011759877204895, 0.5885576605796814, 0.4938037693500519, 0.48775559663772583, 0.5287609100341797, 0.6163457632064819, 0.48057278990745544, 0.5225471258163452, 0.4621118903160095, 0.486015647649765, 0.5602747797966003, 0.5605686902999878, 0.4666035771369934, 0.5657078623771667, 0.5851835012435913, 0.518683910369873, 0.4741356670856476, 0.39318808913230896, 0.5112157464027405, 0.492049902677536, 0.5851055979728699, 0.5256774425506592, 0.5156459808349609, 0.5445963740348816, 0.4529549777507782, 0.4698929488658905, 0.4608663320541382, 0.4287964701652527, 0.4293176829814911, 0.42443808913230896, 0.4237097203731537, 0.36615189909935, 0.3926299214363098, 0.40114039182662964, 0.5541182160377502, 0.45293116569519043, 0.41800016164779663, 0.5331969261169434, 0.43348395824432373, 0.41469112038612366, 0.46030667424201965, 0.3450356721878052, 0.4762718081474304, 0.38729140162467957, 0.3961009085178375, 0.44980186223983765, 0.3461126685142517, 0.45821261405944824, 0.3168390393257141, 0.4617226719856262, 0.396207332611084, 0.40263602137565613, 0.3548929691314697, 0.3344084918498993, 0.302564412355423, 0.38765770196914673, 0.26885607838630676, 0.38659006357192993, 0.24744078516960144, 0.3382205665111542, 0.36688175797462463, 0.40498146414756775, 0.30527785420417786, 0.279145747423172, 0.2880719006061554, 0.3087324798107147, 0.3565630614757538, 0.3259789049625397, 0.27632808685302734, 0.2711457312107086, 0.3170166313648224, 0.25732094049453735, 0.3530718684196472, 0.49173688888549805, 0.2471189647912979, 0.24983488023281097, 0.36334577202796936, 0.3790237307548523, 0.2719484567642212, 0.3266502320766449, 0.31511545181274414, 0.19129528105258942, 0.2583202123641968, 0.2144322246313095, 0.3362295627593994, 0.28672605752944946, 0.22612972557544708, 0.2248922735452652, 0.3245672583580017, 0.27503475546836853, 0.31876862049102783, 0.3206671476364136, 0.2724979519844055, 0.2649676501750946, 0.23597382009029388, 0.319668173789978, 0.14361755549907684, 0.2854025661945343, 0.20830586552619934, 0.224071204662323, 0.2411872148513794, 0.3445017337799072, 0.31117498874664307, 0.24832837283611298, 0.315583199262619, 0.21406593918800354, 0.1864633560180664, 0.20399121940135956, 0.29494476318359375, 0.14760279655456543, 0.2829861044883728, 0.27303987741470337, 0.1898481398820877, 0.2282513976097107, 0.13531705737113953, 0.15181395411491394, 0.33327051997184753, 0.15682575106620789, 0.18169653415679932, 0.16272783279418945, 0.17204570770263672, 0.2771168649196625, 0.17127397656440735, 0.272928386926651, 0.19926676154136658, 0.13033589720726013, 0.19015420973300934, 0.25632205605506897, 0.11591950058937073, 0.23045769333839417, 0.09886735677719116, 0.16712170839309692, 0.28235653042793274, 0.1198701411485672, 0.12428521364927292, 0.14528411626815796, 0.07400773465633392, 0.1287860870361328, 0.2200457602739334, 0.12398131936788559, 0.09393708407878876, 0.1356496661901474, 0.24044165015220642, 0.21497073769569397, 0.09698983281850815, 0.15874527394771576, 0.06263961642980576, 0.061507292091846466, 0.09281707555055618, 0.16628433763980865, 0.1653028130531311, 0.18444715440273285, 0.28638729453086853, 0.16507354378700256, 0.07365182042121887, 0.15688011050224304, 0.08620542287826538, 0.1250103861093521, 0.11098337918519974, 0.11422328650951385, 0.1920483559370041, 0.056585643440485, 0.04305018484592438, 0.20759856700897217, 0.058504749089479446, 0.0648857057094574, 0.1040298268198967, 0.05139264091849327, 0.21178492903709412, 0.1561291217803955, 0.33857759833335876, 0.18806396424770355, 0.07536549866199493, 0.11319126933813095, 0.20158091187477112, 0.304398775100708, 0.0830613225698471, 0.2018921673297882, 0.10792306810617447, 0.1816084235906601, 0.10155071318149567, 0.03165183216333389, 0.1400793194770813, 0.19552069902420044, 0.10122691094875336, 0.08946038782596588, 0.10661180317401886, 0.07655393332242966, 0.08758118748664856, 0.10106350481510162, 0.08837772905826569, 0.06605979055166245, 0.11902126669883728, 0.09390050917863846, 0.1729404777288437, 0.1470789611339569, 0.24506966769695282, 0.09697219729423523, 0.11792422831058502, 0.1123652458190918, 0.12624454498291016, 0.3502466082572937, 0.04415946081280708, 0.046846017241477966, 0.043120332062244415, 0.11517040431499481, 0.1431117206811905, 0.09132416546344757, 0.2308192104101181, 0.15356604754924774, 0.13542871177196503, 0.045282602310180664, 0.26003298163414, 0.14314857125282288, 0.15342862904071808, 0.11684658378362656, 0.1426011621952057, 0.11516019701957703, 0.14980089664459229, 0.26055169105529785, 0.16278308629989624, 0.12837977707386017, 0.14588764309883118, 0.13724613189697266, 0.0308405552059412, 0.06136684864759445, 0.09953656792640686, 0.20540472865104675, 0.32802262902259827, 0.19923967123031616, 0.020684920251369476, 0.06712943315505981, 0.028098320588469505, 0.30228036642074585, 0.16805821657180786, 0.24804458022117615, 0.1900077760219574, 0.20719434320926666, 0.4509837329387665, 0.06428457796573639, 0.07387156039476395, 0.11963227391242981, 0.14545759558677673, 0.24799323081970215, 0.13704168796539307, 0.03472192957997322, 0.05900420621037483, 0.1842651665210724, 0.18616895377635956, 0.10557743906974792, 0.05176946148276329, 0.0872335284948349, 0.12168213725090027, 0.13674256205558777, 0.06537502259016037, 0.21035368740558624, 0.10643720626831055, 0.1304982751607895, 0.09630072861909866, 0.05616427958011627, 0.0745849460363388, 0.5539452433586121, 0.1472911238670349, 0.18090584874153137, 0.377173513174057, 0.07130179554224014, 0.09817202389240265, 0.12674573063850403, 0.14516714215278625, 0.1775592714548111, 0.36777639389038086, 0.0801793709397316, 0.26180800795555115, 0.14677439630031586, 0.2533718943595886, 0.10345826297998428, 0.07526778429746628, 0.06056845188140869, 0.22674556076526642, 0.1677921563386917, 0.1471424251794815, 0.11237022280693054, 0.4629063308238983, 0.5225374698638916, 0.12372200191020966, 0.09073646366596222, 0.12936899065971375, 0.17422334849834442, 0.21058157086372375, 0.27511268854141235, 0.18957698345184326, 0.35794591903686523, 0.17019250988960266, 0.2863452434539795, 0.1540045142173767, 0.3175327479839325, 0.14165210723876953, 0.3644413650035858, 0.16248562932014465, 0.1402318775653839, 0.24213068187236786, 0.1346747875213623, 0.20851291716098785, 0.24768510460853577, 0.6057395339012146, 0.8033247590065002, 0.14748649299144745, 0.3514840602874756, 0.5130511522293091, 0.45541784167289734, 0.6943585872650146, 0.3167647123336792, 0.5215336680412292, 0.4819413721561432, 0.2958236336708069, 0.28508806228637695, 0.22870944440364838, 0.35679274797439575, 0.24254795908927917, 0.3835247755050659, 0.1572636365890503, 0.4327056407928467, 0.30809250473976135, 0.4810982644557953, 0.18015821278095245, 0.4169691205024719, 0.2658701539039612, 0.23312994837760925, 0.24443461000919342, 0.8104143738746643, 0.49408671259880066, 0.47625911235809326, 1.5034464597702026, 0.6812231540679932, 0.3448626399040222, 0.4160643517971039, 0.7684459090232849, 0.4874666631221771, 0.9315577745437622, 0.7801174521446228, 0.5599806308746338, 0.7855857014656067, 0.7121785879135132, 0.386972039937973, 0.4484328329563141, 0.337918221950531, 0.6408190131187439, 0.5225997567176819, 0.7711214423179626, 0.5668426156044006, 0.7041794061660767, 1.03301203250885, 0.6579940915107727, 0.7241469025611877, 0.7794157862663269, 1.0736403465270996, 1.7726099491119385, 1.70753812789917, 1.1134589910507202, 0.844565749168396, 1.0908278226852417, 0.9679023623466492, 1.5659645795822144, 0.998979926109314, 1.2985260486602783, 1.2543585300445557, 0.8668620586395264, 0.6568690538406372, 0.9715377688407898, 0.652842104434967, 2.1740574836730957, 1.6592540740966797, 1.7309256792068481, 0.87059086561203, 1.4063761234283447, 1.2597969770431519, 1.2916278839111328, 1.1091985702514648, 1.0882759094238281, 2.486947536468506, 1.2123453617095947, 1.6819454431533813, 0.9746751189231873, 1.838986873626709, 2.8830714225769043, 2.379957675933838, 0.7612769603729248, 0.6628962755203247, 1.41350519657135, 1.6807682514190674, 0.6225870251655579, 2.3157119750976562, 1.6473380327224731, 1.4777801036834717, 1.0387279987335205, 2.2695021629333496, 1.3058819770812988, 1.1850123405456543, 3.031748056411743, 0.678666353225708, 1.0541553497314453, 1.3429962396621704, 0.763500452041626, 0.5568851232528687, 0.7875490188598633, 0.834507942199707, 1.0565333366394043, 1.511515498161316, 3.781850576400757, 0.7855550646781921, 1.4787580966949463, 5.352015018463135, 4.1666669845581055, 2.7245700359344482, 1.6387274265289307, 3.588499069213867, 5.249892711639404, 1.137266993522644, 2.4453794956207275, 4.204092502593994, 7.962948799133301, 3.190094470977783, 3.4959511756896973, 2.438058376312256, 1.775981068611145, 1.0389882326126099, 2.984471082687378, 2.747037410736084, 2.347907304763794, 2.164799690246582, 1.992650032043457, 1.5899344682693481, 1.1074190139770508, 1.501572608947754, 2.0523428916931152, 0.7318783402442932, 0.8830440640449524, 4.484652519226074, 2.971003532409668, 1.1485867500305176, 4.1723432540893555, 1.6436151266098022, 1.0822259187698364, 4.136144638061523, 10.375626564025879, 6.120433807373047, 3.8752734661102295, 6.561872959136963, 2.108110189437866, 1.3685529232025146, 2.520871639251709, 6.286360740661621, 5.933624744415283, 5.42515230178833, 3.869075298309326, 4.288522720336914, 4.350000858306885, 3.608499050140381, 1.1643790006637573, 1.8241714239120483, 3.1295101642608643, 5.444894790649414, 2.672290802001953, 3.6268091201782227, 2.4374639987945557, 1.7051129341125488, 5.373114109039307, 4.999145984649658, 11.060897827148438, 2.8224422931671143, 2.3251492977142334, 4.56351900100708, 9.504861831665039, 8.150288581848145, 1.745694637298584, 8.372791290283203, 10.45608139038086, 7.586057186126709, 6.953018665313721, 6.131836891174316, 8.560620307922363, 0.8011544942855835, 2.9735445976257324, 1.4882889986038208, 4.62208366394043, 4.130363941192627, 8.428491592407227, 1.943215012550354, 11.44992446899414, 8.583836555480957, 4.265909194946289, 3.671794891357422, 5.941141605377197, 8.780914306640625, 7.05692195892334, 6.033668518066406, 11.977368354797363, 7.237689971923828, 2.0650949478149414, 0.8130156993865967, 0.9168562293052673, 8.62012767791748, 4.719982147216797, 4.232553958892822, 5.556124210357666, 10.434249877929688, 17.1087703704834, 10.231698989868164, 8.55502986907959, 4.773667335510254, 1.1918646097183228, 7.3474202156066895, 20.835670471191406, 20.446609497070312, 9.274112701416016, 12.041094779968262, 13.539830207824707, 5.640708923339844, 10.825528144836426, 14.683638572692871, 4.2194342613220215, 7.573055267333984, 9.800590515136719, 16.224565505981445, 1.95418119430542, 4.8186354637146, 12.259430885314941, 3.0955424308776855, 8.605391502380371, 17.470701217651367, 20.063745498657227, 4.51368522644043, 7.16865873336792, 23.774967193603516, 2.9923110008239746, 3.398228168487549, 13.034398078918457, 7.35700798034668, 9.276202201843262, 17.55207061767578, 2.943270206451416, 7.5461297035217285, 12.100436210632324, 9.708345413208008, 10.067049026489258, 7.898036003112793, 16.963003158569336, 20.519044876098633, 30.07126235961914, 41.18986511230469, 20.6453914642334, 12.887643814086914, 25.575246810913086, 6.677354335784912, 23.96486473083496, 30.703264236450195, 8.300257682800293, 14.803939819335938, 15.390021324157715, 25.278226852416992, 13.446012496948242, 17.4953556060791, 43.767173767089844, 21.63767433166504, 37.533653259277344, 27.10078239440918, 47.28738021850586, 29.669084548950195, 34.25985336303711, 29.523441314697266, 16.78140640258789, 41.84815979003906, 29.969093322753906, 13.090520858764648, 47.10166931152344, 21.38205909729004, 71.94866180419922, 68.3693618774414, 30.897140502929688, 40.78993606567383, 49.90106964111328, 36.019283294677734, 19.69852066040039, 59.87998580932617, 24.889026641845703, 19.664094924926758, 78.42058563232422, 54.18964385986328, 71.54667663574219, 62.914710998535156, 127.13558959960938, 117.05470275878906, 31.445987701416016, 130.4477081298828, 118.43834686279297, 50.58054733276367, 102.8344955444336, 117.4892807006836, 124.98538970947266, 63.94906234741211, 75.31214141845703, 61.150550842285156, 121.75762176513672, 86.84474182128906, 130.4115447998047, 148.58116149902344, 46.21041488647461, 127.07795715332031, 166.7871856689453, 71.58660888671875, 92.56756591796875, 61.79575729370117, 176.3225860595703], \"type\": \"scatter\", \"uid\": \"dec3ca17-26d3-48c3-873f-21e5c3df69ed\"}], {\"title\": {\"text\": \"batch/loss\"}, \"yaxis\": {\"hoverformat\": \".6f\"}}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"})});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){window._Plotly.Plots.resize(document.getElementById(\"ed7f06c6-5622-4773-a4a4-a9d6891b3a34\"));});</script>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div id=\"07781608-3771-4e99-8da6-8a4b77251e0f\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"07781608-3771-4e99-8da6-8a4b77251e0f\", [{\"name\": \"train/batch/lr\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716], \"y\": [9.999999974752427e-07, 1.0162094667975907e-06, 1.0326816664019134e-06, 1.0494209163880441e-06, 1.0664315368558164e-06, 1.0837177342182258e-06, 1.1012842833224568e-06, 1.1191355042683426e-06, 1.1372761719030677e-06, 1.1557107200133032e-06, 1.1744442645067465e-06, 1.1934813528569066e-06, 1.2128271009714808e-06, 1.2324863973844913e-06, 1.2524643580036354e-06, 1.2727660987366107e-06, 1.2933969628647901e-06, 1.3143622936695465e-06, 1.3356674344322528e-06, 1.3573178421211196e-06, 1.3793193147648708e-06, 1.401677309331717e-06, 1.4243977375372197e-06, 1.4474865110969404e-06, 1.4709495417264407e-06, 1.4947928548281197e-06, 1.5190225894912146e-06, 1.5436452258654754e-06, 1.568666903040139e-06, 1.5940941011649556e-06, 1.6199335277633509e-06, 1.6461918903587502e-06, 1.6728757827877416e-06, 1.699992139947426e-06, 1.72754812410858e-06, 1.755550783855142e-06, 1.7840073951447266e-06, 1.8129252339349478e-06, 1.8423118035570951e-06, 1.872174721029296e-06, 1.902521603369678e-06, 1.9333606360305566e-06, 1.964699322343222e-06, 1.9965459614468273e-06, 2.0289089661673643e-06, 2.061796521957149e-06, 2.0952172690158477e-06, 2.1291796201694524e-06, 2.163692442991305e-06, 2.1987648324284237e-06, 2.2344056560541503e-06, 2.270624236189178e-06, 2.307429667780525e-06, 2.3448319552699104e-06, 2.3828404209780274e-06, 2.4214650693465956e-06, 2.460715677443659e-06, 2.500602704458288e-06, 2.541136154832202e-06, 2.582326715128147e-06, 2.6241848445351934e-06, 2.6667214569897624e-06, 2.7099476938019507e-06, 2.7538744689081796e-06, 2.7985133783658966e-06, 2.843875790858874e-06, 2.889973529818235e-06, 2.9368184186751023e-06, 2.98442273560795e-06, 3.0327985314215766e-06, 3.0819587664154824e-06, 3.1319157187681412e-06, 3.1826823487790534e-06, 3.23427207149507e-06, 3.2866978472156916e-06, 3.33997354573512e-06, 3.394112582100206e-06, 3.4491295082261786e-06, 3.505037966533564e-06, 3.5618529636849416e-06, 3.619588596848189e-06, 3.678260327433236e-06, 3.7378829347289866e-06, 3.7984721075190464e-06, 3.8600433072133455e-06, 3.922612449969165e-06, 3.986196134064812e-06, 4.050810275657568e-06, 4.1164717003994156e-06, 4.183197688689688e-06, 4.251005066180369e-06, 4.319911568018142e-06, 4.3899349293496925e-06, 4.4610937948164064e-06, 4.5334054448176175e-06, 4.606889888236765e-06, 4.681564860220533e-06, 4.7574508243997116e-06, 4.834566425415687e-06, 4.91293212689925e-06, 4.992568392481189e-06, 5.073495231044944e-06, 5.155734015716007e-06, 5.239305664872518e-06, 5.324232006387319e-06, 5.410535322880605e-06, 5.498236987477867e-06, 5.587360647041351e-06, 5.6779290389386006e-06, 5.769964900537161e-06, 5.863493242941331e-06, 5.958537258266006e-06, 6.0551219576154836e-06, 6.1532723520940635e-06, 6.253013907553395e-06, 6.3543716350977775e-06, 6.457372819568263e-06, 6.5620433815638535e-06, 6.668410605925601e-06, 6.77650223224191e-06, 6.8863460001011845e-06, 6.997969649091829e-06, 7.111403192539001e-06, 7.226675279525807e-06, 7.343815923377406e-06, 7.4628555921663065e-06, 7.5838242992176674e-06, 7.706754331593402e-06, 7.83167706686072e-06, 7.958624337334186e-06, 8.087628884823062e-06, 8.218725270126015e-06, 8.35194623505231e-06, 8.487327249895316e-06, 8.624901965958998e-06, 8.764707672526129e-06, 8.906778930395376e-06, 9.051153028849512e-06, 9.19786725717131e-06, 9.346959814138245e-06, 9.498469808022492e-06, 9.652434528106824e-06, 9.808895811147522e-06, 9.967892765416764e-06, 1.012946722767083e-05, 1.02936601251713e-05, 1.046051511366386e-05, 1.0630074939399492e-05, 1.080238234862918e-05, 1.0977483725582715e-05, 1.1155422726005781e-05, 1.1336246643622871e-05, 1.1520000953169074e-05, 1.1706733857863583e-05, 1.1896494470420294e-05, 1.2089330084563699e-05, 1.2285291631997097e-05, 1.2484430044423789e-05, 1.2686796253547072e-05, 1.289244210056495e-05, 1.3101422155159526e-05, 1.3313789168023504e-05, 1.3529598618333694e-05, 1.3748905985266902e-05, 1.397176856698934e-05, 1.419824366166722e-05, 1.4428390386456158e-05, 1.4662266949017067e-05, 1.4899934285494965e-05, 1.5141454241529573e-05, 1.5386889572255313e-05, 1.5636302123311907e-05, 1.5889760106801987e-05, 1.614732354937587e-05, 1.6409063391620293e-05, 1.6675045117153786e-05, 1.6945339666563086e-05, 1.7220014342456125e-05, 1.7499141904409043e-05, 1.778279329300858e-05, 1.8071043086820282e-05, 1.8363965864409693e-05, 1.8661636204342358e-05, 1.896413232316263e-05, 1.927153061842546e-05, 1.95839111256646e-05, 1.990135569940321e-05, 2.022394801315386e-05, 2.0551766283460893e-05, 2.0884899640805088e-05, 2.122343357768841e-05, 2.1567453586612828e-05, 2.1917050617048517e-05, 2.2272315618465655e-05, 2.2633337721345015e-05, 2.3000211513135582e-05, 2.3373033400275744e-05, 2.3751897970214486e-05, 2.41369034483796e-05, 2.4528149879188277e-05, 2.492573912604712e-05, 2.5329773052362725e-05, 2.5740355340531096e-05, 2.6157591491937637e-05, 2.6581592464935966e-05, 2.7012467398890294e-05, 2.7450325433164835e-05, 2.7895279345102608e-05, 2.8347447369014844e-05, 2.880694592022337e-05, 2.9273891414050013e-05, 2.974840572278481e-05, 3.02306107187178e-05, 3.0720631912117824e-05, 3.121859845123254e-05, 3.172463766532019e-05, 3.223887688363902e-05, 3.2761450711404905e-05, 3.32924973918125e-05, 3.383215153007768e-05, 3.438055136939511e-05, 3.4937842428917065e-05, 3.550416658981703e-05, 3.607966937124729e-05, 3.666450356831774e-05, 3.7258814700180665e-05, 3.7862762837903574e-05, 3.847649713861756e-05, 3.910018131136894e-05, 3.973397542722523e-05, 4.037804319523275e-05, 4.103254832443781e-05, 4.169766543782316e-05, 4.2373561882413924e-05, 4.3060415919171646e-05, 4.375840217107907e-05, 4.446770253707655e-05, 4.518850255408324e-05, 4.592098412103951e-05, 4.666533641284332e-05, 4.7421759518329054e-05, 4.8190438974415883e-05, 4.897158214589581e-05, 4.9765385483624414e-05, 5.05720563523937e-05, 5.139180211699568e-05, 5.222483741817996e-05, 5.307137325871736e-05, 5.3931635193293914e-05, 5.4805837862659246e-05, 5.56942104594782e-05, 5.659698581439443e-05, 5.751439312007278e-05, 5.844666884513572e-05, 5.939406037214212e-05, 6.035680780769326e-05, 6.133515853434801e-05, 6.232936721062288e-05, 6.333969940897077e-05, 6.436640251195058e-05, 6.540974572999403e-05, 6.647000554949045e-05, 6.754744390491396e-05, 6.864235911052674e-05, 6.97550131008029e-05, 7.088570419000462e-05, 7.203472341643646e-05, 7.320236909436062e-05, 7.438893953803927e-05, 7.559474761364982e-05, 7.682009891141206e-05, 7.806531357346103e-05, 7.933071174193174e-05, 8.061662083491683e-05, 8.192337554646656e-05, 8.325131057063118e-05, 8.460076787741855e-05, 8.597210398875177e-05, 8.73656608746387e-05, 8.87818168848753e-05, 9.022092126542702e-05, 9.168335964204744e-05, 9.316949581261724e-05, 9.467972267884761e-05, 9.621443314244971e-05, 9.77740201051347e-05, 9.935888374457136e-05, 0.0001009694387903437, 0.00010260609997203574, 0.00010426928929518908, 0.00010595944331726059, 0.0001076769913197495, 0.00010942237713607028, 0.00011119605915155262, 0.00011299848847556859, 0.00011483013076940551, 0.00011669146624626592, 0.00011858297511935234, 0.00012050514487782493, 0.00012245847028680146, 0.0001244434533873573, 0.00012646062532439828, 0.00012851048086304218, 0.00013059357297606766, 0.00013271042553242296, 0.00013486159150488675, 0.0001370476238662377, 0.00013926909014116973, 0.00014152657240629196, 0.0001438206381862983, 0.00014615189866162837, 0.0001485209504608065, 0.00015092839021235704, 0.00015337485820055008, 0.00015586099470965564, 0.00015838741092011333, 0.00016095479077193886, 0.00016356378910131752, 0.0001662150607444346, 0.0001689093332970515, 0.00017164726159535348, 0.000174429573235102, 0.00017725698126014322, 0.0001801302278181538, 0.00018305004050489515, 0.00018601719057187438, 0.00018903243471868336, 0.0001920965441968292, 0.00019521033391356468, 0.00019837458967231214, 0.0002015901409322396, 0.00020485780260059983, 0.00020817844779230654, 0.00021155290596652776, 0.0002149820647900924, 0.00021846681192982942, 0.00022200804960448295, 0.00022560668003279716, 0.0002292636490892619, 0.00023297988809645176, 0.00023675637203268707, 0.0002405940613243729, 0.0002444939746055752, 0.00024845710140652955, 0.0002524844603613019, 0.0002565770992077887, 0.0002607360656838864, 0.00026496246573515236, 0.00026925737620331347, 0.0002736219030339271, 0.00027805715217255056, 0.0002825643168762326, 0.0002871445321943611, 0.0002917989913839847, 0.0002965289168059826, 0.0003013354726135731, 0.0003062199684791267, 0.0003111836558673531, 0.0003162277571391314, 0.0003213536401744932, 0.00032656261464580894, 0.0003318560484331101, 0.0003372352512087673, 0.00034270164906047285, 0.00034825666807591915, 0.0003539017343427986, 0.00035963827394880354, 0.0003654678293969482, 0.0003713918849825859, 0.00037741195410490036, 0.00038352960837073624, 0.0003897464193869382, 0.0003960639878641814, 0.00040248397272080183, 0.00040900803287513554, 0.000415637856349349, 0.0004223751020617783, 0.00042922160355374217, 0.0004361790488474071, 0.0004432492714840919, 0.00045043413410894573, 0.0004577354120556265, 0.0004651550843846053, 0.00047269498463720083, 0.00048035712097771466, 0.0004881434724666178, 0.0004960560472682118, 0.0005040968535467982, 0.0005122679867781699, 0.0005205715424381196, 0.0005290097906254232, 0.0005375847104005516, 0.0005462986882776022, 0.0005551539361476898, 0.0005641526659019291, 0.0005732972640544176, 0.0005825901171192527, 0.0005920336116105318, 0.0006016301922500134, 0.0006113823037594557, 0.0006212924490682781, 0.0006313633057288826, 0.0006415973766706884, 0.0006519973394460976, 0.0006625658716075122, 0.0006733057089149952, 0.0006842196453362703, 0.0006953104748390615, 0.0007065811078064144, 0.0007180343964137137, 0.000729673367459327, 0.0007415009895339608, 0.0007535203476436436, 0.0007657344685867429, 0.0007781466701999307, 0.0007907599792815745, 0.0008035778300836682, 0.0008166033658199012, 0.0008298400789499283, 0.0008432913455180824, 0.0008569606579840183, 0.0008708515670150518, 0.0008849676232784986, 0.0008993124938569963, 0.0009138898458331823, 0.000928703520912677, 0.0009437573025934398, 0.0009590551489964128, 0.0009746009018272161, 0.0009903986938297749, 0.001006452483125031, 0.0010227665770798922, 0.0010393451666459441, 0.0010561923263594508, 0.0010733127128332853, 0.0010907105170190334, 0.0011083903955295682, 0.001126356772147119, 0.0011446144198998809, 0.0011631679954007268, 0.0011820223880931735, 0.001201182371005416, 0.0012206528335809708, 0.0012404390145093203, 0.001260545919649303, 0.0012809786712750793, 0.0013017426244914532, 0.0013228432508185506, 0.0013442857889458537, 0.001366075943224132, 0.0013882194180041552, 0.0014107216848060489, 0.0014335887972265482, 0.0014568264596164227, 0.001480440841987729, 0.0015044379979372025, 0.0015288242138922215, 0.001553605659864843, 0.0015787887386977673, 0.0016043800860643387, 0.0016303862212225795, 0.0016568138962611556, 0.0016836699796840549, 0.0017109614564105868, 0.0017386951949447393, 0.0017668785294517875, 0.0017955187940970063, 0.0018246232066303492, 0.0018541993340477347, 0.0018842549761757255, 0.00191479770001024, 0.0019458356546238065, 0.001977376639842987, 0.0020094288047403097, 0.002042000647634268, 0.0020751003175973892, 0.002108736662194133, 0.00214291806332767, 0.0021776538342237473, 0.002212952356785536, 0.0022488231770694256, 0.002285275375470519, 0.002322318498045206, 0.0023599620908498764, 0.0023982159327715635, 0.0024370895698666573, 0.0024765937123447657, 0.0025167379062622786, 0.002557532861828804, 0.0025989890564233065, 0.0026411174330860376, 0.002683928469195962, 0.0027274335734546185, 0.0027716439217329025, 0.0028165706899017096, 0.0028622259851545095, 0.0029086212161928415, 0.002955768257379532, 0.003003679681569338, 0.003052367828786373, 0.0031018450390547514, 0.0031521243508905172, 0.0032032188028097153, 0.0032551412004977465, 0.0033079052809625864, 0.0033615247812122107, 0.003416013205423951, 0.0034713849890977144, 0.0035276543349027634, 0.003584835911169648, 0.0036429441533982754, 0.003701994428411126, 0.003762001870200038, 0.003822981845587492, 0.0038849504198879004, 0.0039479234255850315, 0.004011916927993298, 0.004076948389410973, 0.004143033642321825, 0.004210189916193485, 0.004278434906154871, 0.004347785841673613, 0.004418261349201202, 0.004489879123866558, 0.004562657792121172, 0.004636615980416536, 0.004711772780865431, 0.00478814821690321, 0.00486576184630394, 0.004944633226841688, 0.0050247833132743835, 0.00510623212903738, 0.005189001560211182, 0.005273112561553717, 0.005358587019145489, 0.005445446819067001, 0.0055337147787213326, 0.005623413249850273, 0.005714565981179476, 0.005807195790112019, 0.005901327356696129, 0.005996984895318747, 0.006094193086028099, 0.006192976608872414, 0.0062933615408837795, 0.006395373493432999, 0.006499039474874735, 0.00660438509657979, 0.00671143876388669, 0.006820227950811386, 0.006930780131369829, 0.00704312464222312, 0.0071572898887097836, 0.007273305673152208, 0.007391202263534069, 0.007511009927839041, 0.007632759399712086, 0.007756482344120741, 0.007882210426032543, 0.008009977638721466, 0.00813981518149376, 0.008271756581962109, 0.008405838161706924, 0.008542091585695744, 0.008680555038154125, 0.008821262046694756, 0.00896424986422062, 0.0091095557436347, 0.009257216937839985, 0.00940727163106203, 0.009559758938848972, 0.009714717045426369, 0.009872187860310078, 0.010032210499048233, 0.010194827802479267, 0.010360080748796463, 0.010528012178838253, 0.010698665864765644, 0.010872085578739643, 0.011048316024243832, 0.01122740376740694, 0.011409393511712551, 0.011594333685934544, 0.011782271787524223, 0.01197325624525547, 0.012167336419224739, 0.012364562600851059, 0.012564986012876034, 0.012768657878041267, 0.012975631281733513, 0.013185959309339523, 0.013399696908891201, 0.013616899028420448, 0.013837621547281742, 0.014061922207474709, 0.014289858750998974, 0.014521489851176739, 0.014756875112652779, 0.014996076934039593, 0.015239154919981956, 0.015486174263060093, 0.015737196430563927, 0.015992287546396255, 0.016251515597105026, 0.016514943912625313, 0.016782641410827637, 0.017054680734872818, 0.01733112707734108, 0.01761205680668354, 0.01789753884077072, 0.018187647685408592, 0.01848245970904827, 0.018782051280140877, 0.019086498767137527, 0.01939588040113449, 0.019710278138518333, 0.020029770210385323, 0.020354442298412323, 0.02068437822163105, 0.02101965993642807, 0.021360378712415695, 0.02170661836862564, 0.02205847203731537, 0.022416027262806892, 0.022779380902647972, 0.02314862236380577, 0.02352384850382805, 0.023905158042907715, 0.02429264783859253, 0.02468641847372055, 0.025086572393774986, 0.025493213906884193, 0.02590644545853138, 0.0263263750821352, 0.02675311081111431, 0.027186766266822815, 0.027627449482679367, 0.02807527594268322, 0.028530361130833626, 0.028992822393774986, 0.029462780803442, 0.02994035743176937, 0.030425675213336945, 0.03091885894536972, 0.031420037150382996, 0.031929340213537216, 0.03244689851999283, 0.03297284618020058, 0.033507317304611206, 0.03405045345425606, 0.034602392464876175, 0.0351632796227932, 0.035733260214328766, 0.03631247580051422, 0.0369010828435421, 0.03749923035502434, 0.038107071071863174, 0.03872476890683174, 0.039352476596832275, 0.03999035805463791, 0.04063858091831207, 0.0412973128259182, 0.041966721415519714, 0.04264697805047035, 0.043338265269994736, 0.044040754437446594, 0.04475463181734085, 0.04548007994890213, 0.046217288821935654, 0.04696644842624664, 0.04772774875164032, 0.0485013909637928, 0.049287572503089905, 0.05008649826049805, 0.05089837312698364, 0.051723409444093704, 0.052561819553375244, 0.05341381952166557, 0.0542796291410923, 0.055159471929073334, 0.05605357885360718, 0.05696217715740204, 0.05788550525903702, 0.05882379785180092, 0.05977730080485344, 0.06074625998735428, 0.06173092499375343, 0.0627315491437912, 0.06374839693307877, 0.06478172540664673, 0.06583180278539658, 0.0668989047408104, 0.06798329949378967, 0.0690852701663971, 0.07020510733127594, 0.07134309411048889, 0.07249952852725983, 0.07367470860481262, 0.07486893981695175, 0.07608252763748169, 0.0773157849907875, 0.07856903225183487, 0.07984259724617004, 0.0811368003487587, 0.08245198428630829, 0.08378849178552628, 0.08514665812253952, 0.08652684092521667, 0.0879293903708458, 0.08935468643903732, 0.09080307185649872, 0.09227494895458221, 0.09377067536115646, 0.0952906459569931, 0.09683525562286377, 0.0984049066901207, 0.10000000149011612], \"type\": \"scatter\", \"uid\": \"7893a5a1-6974-419b-965c-0e4fff1a0076\"}], {\"title\": {\"text\": \"batch/lr\"}, \"yaxis\": {\"hoverformat\": \".6f\"}}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"})});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){window._Plotly.Plots.resize(document.getElementById(\"07781608-3771-4e99-8da6-8a4b77251e0f\"));});</script>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Zoom in plot to see on which step the loss was still decreasing\n",
    "# and choose LR from this step\n",
    "keker.plot_kek_lr(logdir=\"/path/to/logdir\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Kek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100% 717/717 [02:17<00:00,  6.00it/s, loss=0.1613, val_loss=0.0292, acc=0.9881]\n",
      "Epoch 2/3: 100% 717/717 [02:16<00:00,  5.35it/s, loss=0.1650, val_loss=0.0292, acc=0.9891]\n",
      "Epoch 3/3: 100% 717/717 [02:16<00:00,  5.21it/s, loss=0.1229, val_loss=0.0259, acc=0.9906]\n"
     ]
    }
   ],
   "source": [
    "# Ok, now let's start training!\n",
    "# It's as simple as:\n",
    "keker.kek(lr=1e-5, epochs=3)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kek with different optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100% 717/717 [02:15<00:00,  5.42it/s, loss=0.1829, val_loss=0.0387, acc=0.9916]\n"
     ]
    }
   ],
   "source": [
    "# SomeKekasUser: Wait, and what if I want to train with the different optimizer?\n",
    "#\n",
    "# Me:\n",
    "keker.kek(lr=1e-5, \n",
    "          epochs=1,\n",
    "          opt=torch.optim.RMSprop,            # optimizer class\n",
    "          opt_params={\"weight_decay\": 1e-5})  # optimizer kwargs in dict format (if you want)\n",
    "\n",
    "# by default, the optimizer specified on Keker initialization is used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kek with scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100% 717/717 [02:16<00:00,  6.00it/s, loss=0.1569, val_loss=0.0298, acc=0.9876]\n",
      "Epoch 2/2: 100% 717/717 [02:17<00:00,  5.98it/s, loss=0.1267, val_loss=0.0272, acc=0.9881]\n"
     ]
    }
   ],
   "source": [
    "# SomeKekasUser: OK, and what if I want to use a pytorch scheduler?\n",
    "#\n",
    "# Me:\n",
    "keker.kek(lr=1e-5,\n",
    "          epochs=2,\n",
    "          sched=torch.optim.lr_scheduler.StepLR,       # pytorch lr scheduler class\n",
    "          sched_params={\"step_size\":1, \"gamma\": 0.9})  # schedulres kwargas in dict format\n",
    "\n",
    "# by default, no scheduler is using"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log your keks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100% 717/717 [02:47<00:00,  4.43it/s, loss=0.0520, val_loss=0.0258, acc=0.9911]\n"
     ]
    }
   ],
   "source": [
    "# SomeKekasUser: How about logging?\n",
    "#\n",
    "# Me:\n",
    "keker.kek(lr=1e-5,\n",
    "          epochs=1,\n",
    "          logdir=\"/mnt/hdd3_4/belskikh/keks/forplot\")\n",
    "\n",
    "# It will create a `train` and `val` subfolders in logdir, and will write tensorboard logs into them\n",
    "# to see them start a tensorboard with `--logdir /path/to/logdir`\n",
    "# OR you can use keker.plot_kek method! (see cell below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot your keks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div id=\"58699014-49d6-430d-a006-c1b3c7dfadfd\" style=\"height: 400px; width: 800px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"58699014-49d6-430d-a006-c1b3c7dfadfd\", [{\"name\": \"train/batch/loss\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716], \"y\": [0.7217793464660645, 0.6225721836090088, 0.6978440880775452, 0.587955117225647, 0.6213288307189941, 0.49810636043548584, 0.4819848835468292, 0.43636730313301086, 0.4905984103679657, 0.5085959434509277, 0.48953601717948914, 0.3726704716682434, 0.3063039183616638, 0.37848109006881714, 0.34461838006973267, 0.30345419049263, 0.27994194626808167, 0.3682718276977539, 0.33093026280403137, 0.25545552372932434, 0.32020556926727295, 0.24079342186450958, 0.3132065236568451, 0.21423764526844025, 0.15670230984687805, 0.2203402817249298, 0.211973175406456, 0.27562451362609863, 0.21372036635875702, 0.13162943720817566, 0.16005736589431763, 0.33191150426864624, 0.11865301430225372, 0.23960956931114197, 0.15579771995544434, 0.22314223647117615, 0.17381365597248077, 0.3114203214645386, 0.2138608694076538, 0.15137429535388947, 0.16940946877002716, 0.16962626576423645, 0.09658224135637283, 0.14785915613174438, 0.2181808352470398, 0.15600508451461792, 0.14961591362953186, 0.20538654923439026, 0.2232740968465805, 0.1067865863442421, 0.15577003359794617, 0.12279721349477768, 0.14359058439731598, 0.21166467666625977, 0.11489678919315338, 0.31785547733306885, 0.20975297689437866, 0.11374863237142563, 0.2663699686527252, 0.13794581592082977, 0.17912660539150238, 0.1805843710899353, 0.15339595079421997, 0.2006910741329193, 0.09032171964645386, 0.07388384640216827, 0.13707183301448822, 0.09115727245807648, 0.2675592005252838, 0.1440429538488388, 0.146877259016037, 0.11520548909902573, 0.04841354489326477, 0.10899919271469116, 0.06170038878917694, 0.06006431579589844, 0.19306939840316772, 0.23731747269630432, 0.09337250143289566, 0.10810890048742294, 0.21711426973342896, 0.1100434809923172, 0.016915161162614822, 0.12618938088417053, 0.1131037101149559, 0.07750087231397629, 0.27277523279190063, 0.16785095632076263, 0.21842831373214722, 0.061992332339286804, 0.10934710502624512, 0.12831613421440125, 0.15226992964744568, 0.028218738734722137, 0.18274396657943726, 0.09332703053951263, 0.10043272376060486, 0.17980220913887024, 0.14383280277252197, 0.1608734428882599, 0.09573738276958466, 0.038418322801589966, 0.09567863494157791, 0.0390617698431015, 0.13328400254249573, 0.08099537342786789, 0.3842967748641968, 0.09308013319969177, 0.03776349872350693, 0.024325303733348846, 0.07056199014186859, 0.050703663378953934, 0.21455684304237366, 0.2032465934753418, 0.09999296069145203, 0.10649829357862473, 0.2278975248336792, 0.11814459413290024, 0.12163309752941132, 0.06133277714252472, 0.10768505185842514, 0.1013977974653244, 0.046252042055130005, 0.07484840601682663, 0.09272311627864838, 0.10210969299077988, 0.04259331524372101, 0.1284707486629486, 0.0703786164522171, 0.1151985228061676, 0.13442495465278625, 0.018825789913535118, 0.052119240164756775, 0.09264199435710907, 0.07555244117975235, 0.0874786227941513, 0.05544169619679451, 0.1610630750656128, 0.07100102305412292, 0.09065772593021393, 0.010931473225355148, 0.02545124851167202, 0.04966985806822777, 0.04196677729487419, 0.4784427881240845, 0.2058006376028061, 0.045143257826566696, 0.05786290764808655, 0.030668102204799652, 0.1309022605419159, 0.029161624610424042, 0.03189428150653839, 0.06642050296068192, 0.10363038629293442, 0.08111138641834259, 0.0607120580971241, 0.24454812705516815, 0.06957042217254639, 0.037925589829683304, 0.12990455329418182, 0.3978528380393982, 0.18707841634750366, 0.05066714435815811, 0.24754109978675842, 0.09137967228889465, 0.08692362904548645, 0.18890170753002167, 0.0258418507874012, 0.06593307107686996, 0.045626863837242126, 0.06569043546915054, 0.07760367542505264, 0.026875073090195656, 0.2090066373348236, 0.09027963876724243, 0.0763099268078804, 0.1174093559384346, 0.065375916659832, 0.04164563864469528, 0.06364364176988602, 0.09290194511413574, 0.06261510401964188, 0.033873483538627625, 0.03680379316210747, 0.03275471553206444, 0.07412328571081161, 0.0567459762096405, 0.10743013024330139, 0.0969066470861435, 0.2281804382801056, 0.017626652494072914, 0.012738203629851341, 0.015575559809803963, 0.11835417151451111, 0.0986006036400795, 0.006339792162179947, 0.1483137011528015, 0.3201732337474823, 0.04944086819887161, 0.0387614443898201, 0.04825533181428909, 0.022231362760066986, 0.022299211472272873, 0.2585846781730652, 0.02847706712782383, 0.026263458654284477, 0.03899476304650307, 0.0831281989812851, 0.10613861680030823, 0.0903744027018547, 0.01707524061203003, 0.13067786395549774, 0.16956022381782532, 0.22340229153633118, 0.3217194676399231, 0.04673216491937637, 0.3652983009815216, 0.0883830189704895, 0.04939400777220726, 0.04535945504903793, 0.14774009585380554, 0.0725182443857193, 0.062381353229284286, 0.03902239724993706, 0.3332993984222412, 0.20310652256011963, 0.11034402251243591, 0.1489650309085846, 0.045397285372018814, 0.3247321844100952, 0.026737214997410774, 0.05616374313831329, 0.06379741430282593, 0.022927382960915565, 0.20761780440807343, 0.11331072449684143, 0.08383960276842117, 0.051455359905958176, 0.020978862419724464, 0.030546685680747032, 0.011161839589476585, 0.13620148599147797, 0.04775245115160942, 0.029108885675668716, 0.11576271057128906, 0.08877076208591461, 0.13572168350219727, 0.0749359279870987, 0.04473988339304924, 0.28296077251434326, 0.1970689296722412, 0.13436292111873627, 0.0222740788012743, 0.06950093060731888, 0.17352266609668732, 0.15070196986198425, 0.029648294672369957, 0.035507991909980774, 0.1616286039352417, 0.2155398279428482, 0.0694604218006134, 0.2432999163866043, 0.03290475904941559, 0.14753717184066772, 0.06958283483982086, 0.06528251618146896, 0.08007968962192535, 0.015742138028144836, 0.019706720486283302, 0.054053016006946564, 0.11474727094173431, 0.014093130826950073, 0.031892333179712296, 0.027156516909599304, 0.1317225694656372, 0.06752678751945496, 0.03900069370865822, 0.06345707178115845, 0.023321334272623062, 0.15451078116893768, 0.04328662529587746, 0.07604745030403137, 0.016033854335546494, 0.19128240644931793, 0.16702713072299957, 0.02737908996641636, 0.15051855146884918, 0.012079421430826187, 0.03681652247905731, 0.409262090921402, 0.10730969905853271, 0.3346148729324341, 0.09957753866910934, 0.008242722600698471, 0.17240864038467407, 0.08313728123903275, 0.0439755879342556, 0.20477129518985748, 0.11575385928153992, 0.08941731601953506, 0.07799643278121948, 0.077368825674057, 0.18572361767292023, 0.18483000993728638, 0.02753395028412342, 0.13563820719718933, 0.08641105145215988, 0.012553013861179352, 0.03665165603160858, 0.037355612963438034, 0.07988718897104263, 0.2114018201828003, 0.03079240396618843, 0.2770162522792816, 0.252833753824234, 0.06253081560134888, 0.07600955665111542, 0.15676434338092804, 0.061180856078863144, 0.23579472303390503, 0.19687411189079285, 0.060355935245752335, 0.046777453273534775, 0.16985909640789032, 0.06458975374698639, 0.04876706004142761, 0.09116525948047638, 0.06657242774963379, 0.03962886333465576, 0.033182255923748016, 0.2025257647037506, 0.0974765494465828, 0.017073966562747955, 0.014347579330205917, 0.09489721059799194, 0.10175035893917084, 0.1582041084766388, 0.04286704584956169, 0.10406171530485153, 0.03274470567703247, 0.07330533862113953, 0.06273166090250015, 0.06094963848590851, 0.18070794641971588, 0.21572521328926086, 0.021897487342357635, 0.16320794820785522, 0.017941199243068695, 0.039847373962402344, 0.10890816897153854, 0.007900606840848923, 0.061181437224149704, 0.12134100496768951, 0.16027453541755676, 0.09506602585315704, 0.07187937945127487, 0.1280844658613205, 0.13015064597129822, 0.11402595043182373, 0.3186792731285095, 0.029786234721541405, 0.26066017150878906, 0.05143338441848755, 0.2276441752910614, 0.1766487956047058, 0.07228449732065201, 0.03093682788312435, 0.07537119835615158, 0.0625363364815712, 0.275901734828949, 0.035415492951869965, 0.055327631533145905, 0.1412748098373413, 0.09126640856266022, 0.023594554513692856, 0.25648611783981323, 0.12361922115087509, 0.01085016131401062, 0.05208171531558037, 0.10187196731567383, 0.530726432800293, 0.10466616600751877, 0.053488295525312424, 0.03471823036670685, 0.2214258313179016, 0.1112656369805336, 0.5078315138816833, 0.10810878872871399, 0.07213938981294632, 0.1922307163476944, 0.04231506586074829, 0.05431341007351875, 0.24475745856761932, 0.04297247529029846, 0.08822374045848846, 0.0456145741045475, 0.3027816116809845, 0.2109764665365219, 0.029453912749886513, 0.10708006471395493, 0.1687518060207367, 0.0627966970205307, 0.14514264464378357, 0.2298281490802765, 0.015009177848696709, 0.04594779014587402, 0.06730374693870544, 0.07296241074800491, 0.04860490560531616, 0.09136572480201721, 0.12473886460065842, 0.10941272974014282, 0.08518201857805252, 0.18564513325691223, 0.012567687779664993, 0.06940282881259918, 0.08479509502649307, 0.043807707726955414, 0.11155940592288971, 0.19257190823554993, 0.3920450806617737, 0.35457390546798706, 0.030427098274230957, 0.036027420312166214, 0.09037508815526962, 0.05372236296534538, 0.07602794468402863, 0.04446619749069214, 0.07965824753046036, 0.11500828713178635, 0.061176612973213196, 0.09562412649393082, 0.054031264036893845, 0.011598087847232819, 0.04121469333767891, 0.05039482191205025, 0.3348293602466583, 0.20595082640647888, 0.09764962643384933, 0.025785226374864578, 0.05702626705169678, 0.04232180863618851, 0.11429349333047867, 0.07961401343345642, 0.10554107278585434, 0.053740307688713074, 0.18541276454925537, 0.08695246279239655, 0.1052296832203865, 0.17901411652565002, 0.07188760489225388, 0.27179598808288574, 0.011945892125368118, 0.2506996989250183, 0.021662097424268723, 0.059842661023139954, 0.12935540080070496, 0.10523445904254913, 0.04578108340501785, 0.06474003940820694, 0.07259798049926758, 0.02182837575674057, 0.017388368025422096, 0.04594263434410095, 0.23323069512844086, 0.04299239441752434, 0.10532078891992569, 0.15965068340301514, 0.06308385729789734, 0.027134720236063004, 0.0649079754948616, 0.02630739100277424, 0.08870446681976318, 0.1377289742231369, 0.17342938482761383, 0.12352575361728668, 0.019077014178037643, 0.09416742622852325, 0.0413995087146759, 0.06560931354761124, 0.09588495641946793, 0.11529455333948135, 0.025313816964626312, 0.3055725693702698, 0.018821995705366135, 0.07647766917943954, 0.048504654318094254, 0.05817214399576187, 0.0418693982064724, 0.08107329905033112, 0.040231384336948395, 0.06073688715696335, 0.020922904834151268, 0.03862156718969345, 0.3309692442417145, 0.08039228618144989, 0.012462018057703972, 0.17923104763031006, 0.43022245168685913, 0.07773536443710327, 0.03993929550051689, 0.04910711571574211, 0.07300052046775818, 0.12010687589645386, 0.11454316973686218, 0.11957132816314697, 0.026965158060193062, 0.22204118967056274, 0.02845819666981697, 0.1418595016002655, 0.043346088379621506, 0.11316117644309998, 0.15233972668647766, 0.022449230775237083, 0.07157717645168304, 0.043348658829927444, 0.026589034125208855, 0.015769273042678833, 0.08518586307764053, 0.023987559601664543, 0.17162472009658813, 0.013156715780496597, 0.04682385176420212, 0.23564182221889496, 0.04264852777123451, 0.025776267051696777, 0.04960532486438751, 0.027068274095654488, 0.03842076659202576, 0.15304453670978546, 0.07444857805967331, 0.013596776872873306, 0.11171603202819824, 0.07741602510213852, 0.01923515275120735, 0.03348255529999733, 0.01648569665849209, 0.058985576033592224, 0.13663828372955322, 0.07786422967910767, 0.16486448049545288, 0.013677369803190231, 0.12044272571802139, 0.05818741023540497, 0.071670301258564, 0.025954464450478554, 0.0094229057431221, 0.03165307641029358, 0.10811761021614075, 0.10203933715820312, 0.012659374624490738, 0.05514000356197357, 0.10032874345779419, 0.02487695962190628, 0.020522495731711388, 0.12675593793392181, 0.09025850892066956, 0.07436325401067734, 0.039383143186569214, 0.1161181777715683, 0.049581896513700485, 0.05118657648563385, 0.11317094415426254, 0.027848931029438972, 0.017563067376613617, 0.022157663479447365, 0.14626169204711914, 0.02057790756225586, 0.1339944750070572, 0.04410956799983978, 0.019960584118962288, 0.08988458663225174, 0.03757220879197121, 0.03989645466208458, 0.004714209586381912, 0.11929486691951752, 0.01368667371571064, 0.05733586847782135, 0.1430627703666687, 0.1988578885793686, 0.0700206533074379, 0.09328386187553406, 0.34705737233161926, 0.04342213273048401, 0.12467518448829651, 0.08738993853330612, 0.17767342925071716, 0.015713583678007126, 0.04300413653254509, 0.05889921262860298, 0.17492999136447906, 0.1547793447971344, 0.02060607448220253, 0.004253435879945755, 0.03609652817249298, 0.08543660491704941, 0.277057945728302, 0.17462089657783508, 0.07520223408937454, 0.018264884129166603, 0.04103432223200798, 0.028316911309957504, 0.0827634334564209, 0.005127910524606705, 0.009751293808221817, 0.35018301010131836, 0.07997328788042068, 0.10406413674354553, 0.04534009099006653, 0.05427398905158043, 0.06012948602437973, 0.04418113827705383, 0.10669933259487152, 0.3051298260688782, 0.003995295614004135, 0.06782510876655579, 0.089939184486866, 0.031602539122104645, 0.016911828890442848, 0.09444092214107513, 0.026442183181643486, 0.04437138885259628, 0.044396258890628815, 0.05751247704029083, 0.03650408610701561, 0.08204008638858795, 0.12584032118320465, 0.032496094703674316, 0.038549527525901794, 0.10611014068126678, 0.16940446197986603, 0.10756313800811768, 0.10569687932729721, 0.04222641885280609, 0.06910908222198486, 0.006983563303947449, 0.042756885290145874, 0.12711921334266663, 0.05966233089566231, 0.12434147298336029, 0.023909833282232285, 0.0394241139292717, 0.03957851976156235, 0.12328743189573288, 0.01145278662443161, 0.04790312796831131, 0.019976168870925903, 0.04629463702440262, 0.0666390210390091, 0.02941681444644928, 0.08782528340816498, 0.3276152014732361, 0.04119928553700447, 0.03260160610079765, 0.008903777226805687, 0.059989988803863525, 0.057957351207733154, 0.0422540046274662, 0.12439100444316864, 0.12228930741548538, 0.1686110496520996, 0.005186315625905991, 0.07814574986696243, 0.034175753593444824, 0.07769641280174255, 0.015242412686347961, 0.4241172671318054, 0.009469900280237198, 0.326743483543396, 0.11903128772974014, 0.08029402047395706, 0.04809292033314705, 0.2242080122232437, 0.05390498787164688, 0.023994099348783493, 0.026464009657502174, 0.0140773244202137, 0.08310087025165558, 0.01719966158270836, 0.10402751713991165, 0.053526803851127625, 0.059619829058647156, 0.008121788501739502, 0.028833257034420967, 0.054895468056201935, 0.03303901106119156, 0.061460498720407486, 0.23611393570899963, 0.02646573632955551, 0.009684734046459198, 0.06703522056341171, 0.2694382667541504, 0.08234872668981552, 0.20317183434963226, 0.04078032821416855, 0.06375174969434738, 0.008688710629940033, 0.01776033826172352, 0.0077593885362148285, 0.07736074179410934, 0.050937019288539886, 0.030440155416727066, 0.04512248560786247, 0.006646730005741119, 0.026114890351891518, 0.01706414483487606, 0.05503002926707268, 0.03297707438468933, 0.05876314640045166, 0.0381108783185482, 0.06250185519456863, 0.25249820947647095, 0.047885503619909286, 0.007458116859197617, 0.038807131350040436, 0.028031399473547935, 0.012562289834022522], \"type\": \"scatter\", \"uid\": \"59d25a30-8787-4762-bbdb-004a7655525a\"}, {\"name\": \"val/batch/loss\", \"x\": [717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779], \"y\": [0.011762119829654694, 0.0053032152354717255, 0.10081913322210312, 0.004017733037471771, 0.02648678794503212, 0.009344728663563728, 0.003571081906557083, 0.036255352199077606, 0.005081433802843094, 0.0024009495973587036, 0.0021967440843582153, 0.0013925284147262573, 0.003008827567100525, 0.009630491957068443, 0.029039597138762474, 0.004709344357252121, 0.005851682275533676, 0.08654328435659409, 0.0076372940093278885, 0.01842561922967434, 0.002778269350528717, 0.10068457573652267, 0.011648165062069893, 0.3042339086532593, 0.015937360003590584, 0.002390880137681961, 0.0031821243464946747, 0.03113628551363945, 0.04409954324364662, 0.004910852760076523, 0.0030130334198474884, 0.0005682483315467834, 0.017620813101530075, 0.012880226597189903, 0.07671058177947998, 0.02834966592490673, 0.006742868572473526, 0.009629784151911736, 0.021133294329047203, 0.07666769623756409, 0.003491729497909546, 0.04027656465768814, 0.03568436950445175, 0.0016228258609771729, 0.09990900754928589, 0.008440552279353142, 0.004241082817316055, 0.002123359590768814, 0.03865324333310127, 0.0022706426680088043, 0.0013071373105049133, 0.04628948122262955, 0.001163765788078308, 0.0009881556034088135, 0.08684352785348892, 0.01751631125807762, 0.002629302442073822, 0.0012621134519577026, 0.06902575492858887, 0.0016440227627754211, 0.0024302229285240173, 0.008055019192397594, 0.0014170557260513306], \"type\": \"scatter\", \"uid\": \"8575f165-0a06-4505-8a59-218b196872bb\"}], {\"height\": 400, \"title\": {\"text\": \"batch/loss\"}, \"width\": 800, \"yaxis\": {\"hoverformat\": \".6f\"}}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div id=\"dad0c437-3b4f-408b-9975-bea78cb19d11\" style=\"height: 400px; width: 800px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"dad0c437-3b4f-408b-9975-bea78cb19d11\", [{\"name\": \"train/batch/acc\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716], \"y\": [0.5625, 0.59375, 0.5625, 0.6875, 0.6875, 0.8125, 0.8125, 0.875, 0.75, 0.6875, 0.78125, 0.84375, 0.90625, 0.875, 0.90625, 0.90625, 0.9375, 0.8125, 0.875, 0.875, 0.90625, 0.90625, 0.84375, 0.96875, 1.0, 0.875, 0.9375, 0.84375, 0.90625, 0.96875, 0.9375, 0.84375, 1.0, 0.875, 0.9375, 0.875, 0.9375, 0.84375, 0.9375, 1.0, 0.9375, 0.90625, 0.96875, 0.96875, 0.875, 0.9375, 0.90625, 0.90625, 0.875, 1.0, 0.90625, 0.96875, 0.90625, 0.90625, 0.9375, 0.75, 0.9375, 0.9375, 0.90625, 0.96875, 0.875, 0.90625, 0.96875, 0.90625, 0.96875, 1.0, 0.9375, 0.9375, 0.90625, 0.9375, 0.90625, 0.9375, 1.0, 0.9375, 1.0, 0.96875, 0.9375, 0.875, 1.0, 0.96875, 0.9375, 0.9375, 1.0, 0.875, 0.9375, 0.96875, 0.875, 0.96875, 0.9375, 0.96875, 0.96875, 0.9375, 0.90625, 1.0, 0.875, 1.0, 0.96875, 0.9375, 0.90625, 0.90625, 0.9375, 1.0, 1.0, 1.0, 0.9375, 0.96875, 0.875, 0.9375, 1.0, 1.0, 1.0, 1.0, 0.875, 0.9375, 0.96875, 0.96875, 0.875, 0.9375, 0.9375, 0.96875, 0.9375, 0.9375, 0.96875, 0.96875, 0.96875, 0.9375, 1.0, 0.96875, 0.96875, 0.90625, 0.9375, 1.0, 1.0, 0.96875, 0.96875, 0.90625, 1.0, 0.90625, 0.96875, 0.96875, 1.0, 1.0, 1.0, 1.0, 0.84375, 0.9375, 0.96875, 0.96875, 1.0, 0.9375, 1.0, 1.0, 0.96875, 0.96875, 0.96875, 0.96875, 0.90625, 0.96875, 0.96875, 0.96875, 0.875, 0.875, 1.0, 0.90625, 0.96875, 0.9375, 0.90625, 1.0, 0.9375, 0.96875, 0.96875, 0.9375, 1.0, 0.90625, 0.96875, 0.96875, 0.9375, 0.96875, 0.96875, 0.96875, 0.96875, 0.96875, 0.96875, 1.0, 0.96875, 0.96875, 0.9375, 0.9375, 0.9375, 0.9375, 1.0, 1.0, 1.0, 0.9375, 0.96875, 1.0, 0.9375, 0.90625, 0.96875, 0.96875, 0.96875, 1.0, 1.0, 0.9375, 1.0, 1.0, 1.0, 0.96875, 0.9375, 0.96875, 1.0, 0.9375, 0.90625, 0.90625, 0.84375, 1.0, 0.90625, 0.9375, 1.0, 0.96875, 0.9375, 0.96875, 0.96875, 1.0, 0.90625, 0.9375, 0.96875, 0.9375, 1.0, 0.90625, 1.0, 1.0, 0.96875, 1.0, 0.875, 0.9375, 0.9375, 1.0, 1.0, 0.96875, 1.0, 0.9375, 1.0, 1.0, 0.96875, 0.96875, 0.9375, 0.96875, 1.0, 0.875, 0.90625, 0.96875, 1.0, 0.96875, 0.90625, 0.90625, 1.0, 1.0, 0.9375, 0.90625, 0.96875, 0.90625, 1.0, 0.9375, 0.96875, 0.96875, 0.96875, 1.0, 1.0, 0.96875, 0.9375, 1.0, 1.0, 1.0, 0.90625, 0.96875, 1.0, 0.96875, 1.0, 0.9375, 0.96875, 0.9375, 1.0, 0.90625, 0.875, 1.0, 0.9375, 1.0, 0.96875, 0.90625, 0.96875, 0.90625, 0.9375, 1.0, 0.96875, 0.96875, 0.96875, 0.9375, 0.96875, 0.9375, 0.96875, 0.96875, 0.9375, 0.90625, 1.0, 0.9375, 0.9375, 1.0, 1.0, 1.0, 0.96875, 0.90625, 1.0, 0.875, 0.9375, 1.0, 0.96875, 0.96875, 0.96875, 0.90625, 0.9375, 0.96875, 0.96875, 0.9375, 0.96875, 0.96875, 0.9375, 0.96875, 0.96875, 1.0, 0.9375, 0.90625, 1.0, 1.0, 0.96875, 0.9375, 0.9375, 0.96875, 0.96875, 1.0, 0.96875, 0.96875, 0.9375, 0.9375, 0.9375, 1.0, 0.9375, 1.0, 0.96875, 0.96875, 1.0, 0.96875, 0.9375, 0.9375, 0.96875, 0.96875, 0.9375, 0.96875, 0.96875, 0.875, 1.0, 0.9375, 0.96875, 0.90625, 0.90625, 0.96875, 1.0, 0.96875, 0.96875, 0.90625, 1.0, 0.96875, 0.90625, 0.9375, 1.0, 0.9375, 0.9375, 1.0, 0.96875, 0.9375, 0.8125, 0.9375, 0.96875, 0.96875, 0.9375, 0.9375, 0.8125, 0.96875, 0.96875, 0.875, 0.96875, 0.96875, 0.875, 0.96875, 0.96875, 1.0, 0.9375, 0.90625, 1.0, 0.90625, 0.875, 0.96875, 0.96875, 0.9375, 1.0, 1.0, 0.96875, 0.96875, 0.96875, 0.9375, 0.9375, 0.9375, 0.9375, 0.90625, 1.0, 0.9375, 0.90625, 0.96875, 0.9375, 0.90625, 0.9375, 0.8125, 1.0, 1.0, 0.9375, 1.0, 0.9375, 1.0, 0.96875, 0.9375, 1.0, 0.96875, 0.96875, 1.0, 0.96875, 0.96875, 0.90625, 0.9375, 0.9375, 1.0, 1.0, 0.96875, 0.96875, 0.96875, 0.9375, 1.0, 0.9375, 0.96875, 0.96875, 0.9375, 0.96875, 0.9375, 1.0, 0.875, 1.0, 1.0, 0.90625, 0.96875, 1.0, 0.96875, 0.96875, 1.0, 1.0, 1.0, 0.96875, 1.0, 0.9375, 0.96875, 1.0, 1.0, 0.96875, 1.0, 0.9375, 0.9375, 0.9375, 0.96875, 1.0, 0.9375, 1.0, 0.96875, 0.90625, 0.9375, 1.0, 0.9375, 1.0, 0.96875, 0.96875, 0.96875, 1.0, 0.9375, 1.0, 1.0, 1.0, 1.0, 0.9375, 0.96875, 1.0, 0.90625, 0.875, 0.96875, 0.96875, 1.0, 0.9375, 0.96875, 0.96875, 0.96875, 1.0, 0.96875, 1.0, 0.9375, 1.0, 0.9375, 0.875, 1.0, 0.96875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.875, 1.0, 1.0, 0.90625, 1.0, 1.0, 0.96875, 1.0, 1.0, 0.9375, 0.96875, 1.0, 0.9375, 0.96875, 1.0, 1.0, 1.0, 0.96875, 0.9375, 0.96875, 0.9375, 1.0, 0.96875, 0.96875, 0.96875, 1.0, 1.0, 1.0, 0.9375, 0.9375, 1.0, 1.0, 0.96875, 1.0, 1.0, 0.96875, 0.96875, 0.96875, 0.96875, 0.9375, 1.0, 0.96875, 0.9375, 1.0, 1.0, 1.0, 0.9375, 1.0, 0.96875, 0.96875, 1.0, 0.9375, 1.0, 0.96875, 1.0, 0.90625, 1.0, 0.96875, 0.90625, 0.9375, 0.96875, 0.9375, 0.90625, 1.0, 0.9375, 0.96875, 0.9375, 1.0, 0.96875, 0.96875, 0.9375, 0.96875, 1.0, 1.0, 1.0, 0.96875, 0.9375, 0.9375, 0.96875, 1.0, 1.0, 1.0, 0.9375, 1.0, 1.0, 0.9375, 0.9375, 0.96875, 0.96875, 0.96875, 0.96875, 1.0, 0.96875, 0.875, 1.0, 0.96875, 0.9375, 1.0, 1.0, 0.96875, 1.0, 0.96875, 0.96875, 0.96875, 1.0, 0.96875, 0.96875, 1.0, 0.96875, 0.96875, 0.9375, 0.96875, 0.96875, 0.96875, 0.96875, 1.0, 1.0, 0.9375, 0.96875, 0.9375, 1.0, 1.0, 0.96875, 0.9375, 1.0, 0.96875, 1.0, 0.96875, 0.96875, 1.0, 0.9375, 0.9375, 0.96875, 1.0, 1.0, 0.96875, 0.96875, 0.96875, 0.9375, 0.9375, 0.875, 1.0, 0.96875, 1.0, 0.96875, 1.0, 0.875, 1.0, 0.90625, 0.96875, 0.96875, 1.0, 0.90625, 0.96875, 1.0, 1.0, 1.0, 0.96875, 1.0, 0.96875, 0.96875, 0.96875, 1.0, 1.0, 0.96875, 1.0, 0.96875, 0.9375, 1.0, 1.0, 0.96875, 0.875, 0.96875, 0.9375, 1.0, 0.96875, 1.0, 1.0, 1.0, 1.0, 0.96875, 1.0, 0.96875, 1.0, 1.0, 1.0, 0.96875, 1.0, 0.96875, 1.0, 0.9375, 0.90625, 1.0, 1.0, 1.0, 1.0, 1.0], \"type\": \"scatter\", \"uid\": \"8e42c7b2-ad9d-46aa-a8e4-91cd38049eac\"}, {\"name\": \"val/batch/acc\", \"x\": [717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779], \"y\": [1.0, 1.0, 0.9375, 1.0, 0.96875, 1.0, 1.0, 0.96875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96875, 1.0, 1.0, 1.0, 0.96875, 1.0, 0.96875, 1.0, 1.0, 1.0, 0.96875, 0.96875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96875, 0.96875, 1.0, 1.0, 1.0, 0.96875, 1.0, 1.0, 0.96875, 1.0, 0.96875, 1.0, 1.0, 1.0, 0.96875, 1.0, 1.0, 0.96875, 1.0, 1.0, 0.96875, 1.0, 1.0, 1.0, 0.96875, 1.0, 1.0, 1.0, 1.0], \"type\": \"scatter\", \"uid\": \"f1b90a5f-a698-472d-99c8-910f1e066227\"}], {\"height\": 400, \"title\": {\"text\": \"batch/acc\"}, \"width\": 800, \"yaxis\": {\"hoverformat\": \".6f\"}}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div id=\"fae9e1eb-bc5c-4141-aa1e-5343ddff7284\" style=\"height: 400px; width: 800px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"fae9e1eb-bc5c-4141-aa1e-5343ddff7284\", [{\"name\": \"train/batch/lr\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716], \"y\": [4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05, 4.999999873689376e-05], \"type\": \"scatter\", \"uid\": \"7ebe0205-2259-40f4-a0f6-7c0b76a3fd37\"}], {\"height\": 400, \"title\": {\"text\": \"batch/lr\"}, \"width\": 800, \"yaxis\": {\"hoverformat\": \".6f\"}}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# kekas uses plotly lib and tensorboard logs to plot inside NB\n",
    "keker.plot_kek(logdir=\"/path/to/logdir\",  # path to logdir with logs to plot\n",
    "               step=\"batch\",              # (optional) default is \"step\". another option is \"epoch\"\n",
    "                                          # It determines discreteness of ploting\n",
    "               metrics=[\"loss\",           # (optional) list of metrics names\n",
    "                        \"acc\",            # by default [\"loss\", \"lr\"] is using\n",
    "                        \"lr\"],            # the order of the names determines the order of the plot\n",
    "                                          # NOTE: names of metrics must match names in metrics dict\n",
    "                                          # which was specified on Keker init step\n",
    "               height=1200,               # (optional) height of the total plot \n",
    "               width=800)                 # (optional) width of the total plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoints saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100% 717/717 [02:16<00:00,  5.75it/s, loss=0.1404, val_loss=0.0356, acc=0.9901]\n",
      "\n",
      "Checkpoint\tacc\n",
      "/path/to/save/dir/kek.1.h5\t0.990079\n"
     ]
    }
   ],
   "source": [
    "# SomeKekasUser: Also I want to save best checkpoints to later use them for SWA or ensembling!\n",
    "#                And I want to measure them by custom metric, control their number, specify their name prefix,\n",
    "#                and control what I need - minimize or maximize metric!\n",
    "# Me: Here it is:\n",
    "keker.kek(lr=1e-5,\n",
    "          epochs=1,\n",
    "          cp_saver_params={\n",
    "              \"savedir\": \"/path/to/save/dir\",  # a directory for checkpoints\n",
    "              \"metric\": \"acc\",  # (optional) from `metrics` dict on Keker init. \n",
    "                                # default is validation loss\n",
    "              \"n_best\": 3,      # (optional) default is 3\n",
    "              \"prefix\": \"kek\",  # (optional) default prefix is `checkpoint`\n",
    "              \"mode\": \"max\"     # (optional) default is 'min'\n",
    "          })   \n",
    "\n",
    "# It will create a `savedir` directory, and will save best checkpoints there\n",
    "# with naming `{prefix}.{epoch_num}.h5`. The best checkpoint will be dublicated with `{prefix}.best.h5` name\n",
    "# look at the report down here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100% 717/717 [02:17<00:00,  5.78it/s, loss=0.1577, val_loss=0.0341, acc=0.9901]\n"
     ]
    }
   ],
   "source": [
    "# SomeKekasUser: Allright, and I don't want to train model, if validation loss doesn't improve for several epochs.\n",
    "# \n",
    "# Me: You mean, early stopping? Here:\n",
    "keker.kek(lr=1e-5,\n",
    "          epochs=1, \n",
    "          early_stop_params={\n",
    "              \"patience\": 3,   # number of bad epochs to wait before stopping\n",
    "              \"metric\": \"acc\", # (optional) metric name from 'metric' dict. default is val loss\n",
    "              \"mode\": \"min\",   # (optional) what you want from you metric, max or min? default is 'min'\n",
    "              \"min_delta\": 0   # (optional) a minimum delta to count an epoch as 'bad'\n",
    "          })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just do it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100% 717/717 [02:16<00:00,  5.96it/s, loss=0.1697, val_loss=0.0276, acc=0.9926]\n",
      "Epoch 2/5: 100% 717/717 [02:16<00:00,  5.11it/s, loss=0.1241, val_loss=0.0321, acc=0.9901]\n",
      "Epoch 3/5: 100% 717/717 [02:16<00:00,  5.97it/s, loss=0.1576, val_loss=0.0338, acc=0.9901]\n",
      "Epoch 4/5: 100% 717/717 [02:16<00:00,  5.99it/s, loss=0.1733, val_loss=0.0300, acc=0.9906]\n",
      "Epoch 5/5: 100% 717/717 [02:17<00:00,  5.94it/s, loss=0.1397, val_loss=0.0310, acc=0.9916]\n",
      "\n",
      "Checkpoint\tacc\n",
      "/path/to/save/dir/kek.1.h5\t0.992560\n",
      "/path/to/save/dir/kek.5.h5\t0.991567\n",
      "/path/to/save/dir/kek.4.h5\t0.990575\n"
     ]
    }
   ],
   "source": [
    "# SomeAdvancedKekasUser: I WANT IT ALL!\n",
    "# \n",
    "# Me: Well, okay then...\n",
    "keker.kek(lr=1e-5,\n",
    "          epochs=5,\n",
    "          opt=torch.optim.RMSprop,\n",
    "          opt_params={\"weight_decay\": 1e-5},\n",
    "          sched=torch.optim.lr_scheduler.StepLR,\n",
    "          sched_params={\"step_size\":1, \"gamma\": 0.9},\n",
    "          logdir=\"/path/to/logdir\",\n",
    "          cp_saver_params={\n",
    "              \"savedir\": \"/path/to/save/dir\",  \n",
    "              \"metric\": \"acc\",  \n",
    "              \"n_best\": 3,      \n",
    "              \"prefix\": \"kek\",  \n",
    "              \"mode\": \"max\"},     \n",
    "          early_stop_params={\n",
    "              \"patience\": 3,   \n",
    "              \"metric\": \"acc\", \n",
    "              \"mode\": \"min\",   \n",
    "              \"min_delta\": 0\n",
    "          })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Cycle Kek!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100% 717/717 [02:16<00:00,  5.58it/s, loss=0.1120, val_loss=0.0294, acc=0.9906]\n",
      "Epoch 2/5: 100% 717/717 [02:17<00:00,  5.94it/s, loss=0.1517, val_loss=0.0305, acc=0.9906]\n",
      "Epoch 3/5: 100% 717/717 [02:16<00:00,  5.75it/s, loss=0.1507, val_loss=0.0307, acc=0.9901]\n",
      "Epoch 4/5: 100% 717/717 [02:16<00:00,  5.02it/s, loss=0.1187, val_loss=0.0336, acc=0.9906]\n",
      "Epoch 5/5: 100% 717/717 [02:16<00:00,  5.03it/s, loss=0.0963, val_loss=0.0270, acc=0.9911]\n"
     ]
    }
   ],
   "source": [
    "# SomeFastaiFan: Did you stole something else from fastai?\n",
    "#\n",
    "# Me: Yes! One Cycle Policy!\n",
    "keker.kek_one_cycle(max_lr=1e-5,                  # the maximum learning rate\n",
    "                    cycle_len=5,                  # number of epochs, actually, but not exactly\n",
    "                    momentum_range=(0.95, 0.85),  # range of momentum changes\n",
    "                    div_factor=25,                # max_lr / min_lr\n",
    "                    increase_fraction=0.3)        # the part of cycle when learning rate increases\n",
    "\n",
    "# If you don't understand these parameters, read this - https://sgugger.github.io/the-1cycle-policy.html\n",
    "# NOTE: you cannot use schedulers and early stopping with one cycle!\n",
    "# another options are the same as for `kek` method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Keker features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freezing / unfreezing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We've already talk about freezing. But what if I want to unfreeze?\n",
    "# It has the same interface:\n",
    "keker.unfreeze(model_attr=\"net\")\n",
    "\n",
    "# If you want to freeze till some layer:\n",
    "layer_num = -2\n",
    "keker.freeze_to(layer_num, model_attr=\"net\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving / Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving\n",
    "keker.save(\"/path/to/file\")\n",
    "\n",
    "# loading\n",
    "keker.load(\"/path/to/file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device and DataParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keker is using all avialable GPUs by default\n",
    "# To limit it, use 'CUDA_VISIBLE_DEVICES' environment variable (available in os.environ dict)\n",
    "\n",
    "# if you want to specify cuda device for your model, specify `device` parameter on Keker initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are 4 (yes, four) ways to get a predictions with keker\n",
    "\n",
    "# 1st\n",
    "keker.predict(savepath=\"/path/to/save/dir\")\n",
    "# it will makes predicts on your 'test_dl' dataloader (remember, we initialized it with 'None'), if it specified,\n",
    "# and saves models output in numpy.ndarray format to 'savepath'\n",
    "\n",
    "# 2nd\n",
    "loader = val_dl\n",
    "keker.predict_loader(loader=loader, savepath=\"/path/to/save/dir\")\n",
    "# it will do the same as `predict()` but on any custom loader you want\n",
    "\n",
    "# 3rd\n",
    "tensor = torch.zeros(4, 224, 224, 3)\n",
    "preds = keker.predict_tensor(tensor=tensor, to_numpy=False)\n",
    "# it will return a predictions of the model in numpy format if `'to_numpy==True', else - torch.Tensor\n",
    "\n",
    "# 4th\n",
    "array = np.zeros((4, 224, 224, 3))\n",
    "preds = keker.predict_array(array=array, to_numpy=False)\n",
    "# it will do the same as `predict_tensor()` but with np.ndarra as input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict: 100% 63/63 [00:04<00:00, 15.09it/s]\n",
      "Predict: 100% 63/63 [00:05<00:00, 14.98it/s]\n",
      "Predict: 100% 63/63 [00:05<00:00, 15.01it/s]\n"
     ]
    }
   ],
   "source": [
    "# I am sure that it is not very convinient way for test time augmentations,\n",
    "# but here is how you can do it with Kekas\n",
    "\n",
    "# first, specify several augmentations for TTA\n",
    "flip_ = Flip(always_apply=True)\n",
    "vertical_flip_ = VerticalFlip(always_apply=True)\n",
    "transpose_ = Transpose(always_apply=True)\n",
    "\n",
    "# second, create the whole augmentations with theese ones inside\n",
    "def insert_aug(aug, dataset_key=\"image\", size=224):    \n",
    "    PRE_TFMS = Transformer(dataset_key, lambda x: cv2.resize(x, (size, size)))\n",
    "    \n",
    "    AUGS = Transformer(dataset_key, lambda x: aug(image=x)[\"image\"])\n",
    "    \n",
    "    NRM_TFMS = transforms.Compose([\n",
    "        Transformer(dataset_key, to_torch()),\n",
    "        Transformer(dataset_key, normalize())\n",
    "    ])\n",
    "    \n",
    "    tfm = transforms.Compose([PRE_TFMS, AUGS, NRM_TFMS])\n",
    "    return tfm\n",
    "\n",
    "\n",
    "flip = insert_aug(flip_)\n",
    "vertical_flip = insert_aug(vertical_flip_)\n",
    "transpose = insert_aug(transpose_)\n",
    "\n",
    "tta_tfms = {\"flip\": flip, \"v_flip\": vertical_flip, \"transpose\": transpose}\n",
    "\n",
    "# third, run TTA\n",
    "keker.TTA(loader=val_dl,                # loader to predict on \n",
    "          tfms=tta_tfms,                # list or dict of always applying transforms\n",
    "          savedir=\"/path/to/save/dir\",  # savedir\n",
    "          prefix=\"preds\")               # (optional) name prefix. default is 'preds'\n",
    "\n",
    "# it will saves predicts for each augmentation to savedir with name\n",
    "#  - {prefix}_{name_from_dict}.npy if tfms is a dict\n",
    "#  - {prefix}_{index}.npy          if tfms is a list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks is the way in which Kekas customizes its pipeline\n",
    "# each callback implements six methods, which names tell when it applies\n",
    "# on_train_begin()\n",
    "#     on_epoch_begin()\n",
    "#         on_batch_begin()\n",
    "#             >>>... step here ...<<<\n",
    "#         on_batch_end()\n",
    "#     on_epoch_end()\n",
    "# on_train_end()\n",
    "\n",
    "# Callbacks are widely using under the hood of Kekas\n",
    "# For example - loss, opimizer, progressbar, lr scheduling, checkpoint saving, early stopping etc\n",
    "# are realized as callbacks\n",
    "\n",
    "# Callback has access to `state` attr of a keker. Here is a docs from Keker about state:\n",
    "\n",
    "        # The state is an object that stores many variables and represents\n",
    "        # the state of your train-val-repdict pipeline. _state passed to every\n",
    "        # callback call.\n",
    "        # You can use it as a container for your custom variables, but\n",
    "        # DO NOT USE the following ones:\n",
    "        #\n",
    "        # loss, batch, model, dataowner, criterion, opt, parallel, checkpoint,\n",
    "        # stop_iter, stop_epoch, stop_train, out, sched, mode, loader, pbar,\n",
    "        # metrics, epoch_metrics\n",
    "\n",
    "# You can write your own callback, or use something useful from kekas.callbacks\n",
    "\n",
    "# Callbacks should be passes as a list at the Keker initiation\n",
    "# For example, let's use a DebuggerCallback, that just insert a pdb.set_trace() call in pipeline\n",
    "# For more info, please see a DebuggerCallback docs and source code\n",
    "debugger = DebuggerCallback(when=[\"on_epoch_begin\"], modes[\"train\"])\n",
    "\n",
    "keker = Keker(model=model, dataowner=dataowner, criterion=criterion, callbacks=[debugger])\n",
    "\n",
    "# also there is a method to add a callbacks to existing Keker\n",
    "\n",
    "keker.add_callbacks([debugger])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom loss and opimizer callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As was said, loss and optimezer behavior is realiesed as Callbacks.\n",
    "# If you use some tricky loss or optimizer logic, you can create your own Callback\n",
    "# and specify it during Keker initialization\n",
    "\n",
    "# here are the callbacks, that are using by default\n",
    "class LossCallback(Callback):\n",
    "    def __init__(self, target_key: str, preds_key: str) -> None:\n",
    "        # target_key and preds_key are the parameters of Keker\n",
    "        self.target_key = target_key\n",
    "        self.preds_key = preds_key\n",
    "\n",
    "    def on_batch_end(self, i: int, state: DotDict) -> None:\n",
    "        target = state.batch[self.target_key]\n",
    "        preds = state.out[self.preds_key]\n",
    "\n",
    "        state.loss = state.criterion(preds, target)\n",
    "\n",
    "class OptimizerCallback(Callback):\n",
    "    def on_batch_end(self, i: int, state: DotDict) -> None:\n",
    "        if state.mode == \"train\":\n",
    "            state.opt.zero_grad()\n",
    "            state.loss.backward()\n",
    "            state.opt.step()\n",
    "            \n",
    "# and here is how you should specify them during Keker initialization\n",
    "keker = Keker(model=model, \n",
    "              dataowner=dataowner,\n",
    "              criterion=criterion,\n",
    "              loss_cb=LossCallback,\n",
    "              opt_cb=OptimizerCallback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I hope you now got an idea how to use Kekas.\n",
    "\n",
    "I will be happy to get feedback about my library and this tutorial.\n",
    "\n",
    "You can find me in [OpenDataScience](http://ods.ai) community by @belskikh nikname or create an issue on GitHub.\n",
    "\n",
    "Have a good keks!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
